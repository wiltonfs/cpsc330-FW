{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lecture 10: Regression Evaluation Metrics\n",
    "--------------\n",
    "\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi\n",
    " - Office Hours: Mondays 5-6 (or 5-7 if student turn-out was high)\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "- hw4, Feb 10, 11:59pm\n",
    "- Midterm coming up soon on Feb 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-iterating some answer from last session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you calculate precision, recall, f1 score, by default only the positive label is evaluated, assuming **by default that the positive class is labeled 1**. \n",
    "  - In some scikit-learn models, this is configurable through the `pos_label` parameter\n",
    "- When dealing with Multi-class classification, metric are defined per class (and can then be averaged across classes)\n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5jVpe1Q5DbGBFR3rakyig.png width=\"400\"> <img src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPeagrSxOePdyF7zj0u4uw.png width=\"400\">\n",
    "<img src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FB6XcD1Icb7sI8vff1BZuw.png width=\"400\">\n",
    "\n",
    "[image src](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2)\n",
    "\n",
    "    - e.g. recall for cat = 4/(4+1+1) = 4/6 = 0.66\n",
    "- AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "  - What's the probability that a **randomly picked positive sample** has a higher score according to the classifier than a **randomly picked negative sample**. \n",
    "\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Carry out feature transformations on somewhat complicated dataset. \n",
    "- Visualize transformed features as a dataframe. \n",
    "- Use `Ridge` and `RidgeCV`.\n",
    "- Explain how `alpha` hyperparameter of `Ridge` relates to the fundamental tradeoff. \n",
    "- Examine coefficients of transformed features.  \n",
    "- Appropriately select a scoring metric given a regression problem.\n",
    "- Interpret and communicate the meanings of different scoring metrics on regression problems.\n",
    "    - MSE, RMSE, $R^2$, MAPE\n",
    "- Apply log-transform on the target values in a regression problem with `TransformedTargetRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Housing Prices - End2End example\n",
    "\n",
    "In this lecture, we'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/). As usual, to run this notebook you'll need to download the data. For this dataset, train and test have already been separated. We'll be working with the train portion in this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Supervised machine learning problem: predicting **housing price** given features\n",
    "  - Here, the target is `SalePrice`, which is continuous. \n",
    "  - it's a **regression problem** (as opposed to classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train and test `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"SalePrice\"])\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SalePrice\"])\n",
    "y_test = test_df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `pandas_profiler` library\n",
    "\n",
    "We do not have `pandas_profiling` in our course environment. You will have to install it in the environment on your own if you want to run the code below. \n",
    "\n",
    "```pip install -U pandas-profiling[notebook]```\n",
    "\n",
    "The above worked for me. If this doesn't work for you, check the instructions [here](https://pypi.org/project/pandas-profiling/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(train_df, title=\"Pandas Profiling Report\")  # , minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "**Warning on automated EDA tools**\n",
    "\n",
    "- Do not blindly trust all the info given to you by automated tools. \n",
    "- How does pandas profiling figure out the data type?\n",
    "    - You can look at the Python data type and say floats are numeric, strings are categorical.\n",
    "    - However, in doing so you would miss out on various subtleties such as **some of the string features being ordinal** rather than truly categorical.\n",
    "    - Also, it might think **free text data** is **categorical** (which is not right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In addition to tools such as above, it's important to go through data description to understand the data.\n",
    "- The data description for our dataset is available [here](https://www.kaggle.com/c/home-data-for-ml-course/data?select=data_description.txt).     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature types \n",
    "\n",
    "- We have mixed feature types and a bunch of missing values. \n",
    "- Now, let's identify feature types and transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's get the numeric-looking columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_looking_columns = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "print(numeric_looking_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not all numeric looking columns are necessarily numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"MSSubClass\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
    "\n",
    "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "        30\t1-STORY 1945 & OLDER\n",
    "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "        50\t1-1/2 STORY FINISHED ALL AGES\n",
    "        60\t2-STORY 1946 & NEWER\n",
    "        70\t2-STORY 1945 & OLDER\n",
    "        75\t2-1/2 STORY ALL AGES\n",
    "        80\tSPLIT OR MULTI-LEVEL\n",
    "        85\tSPLIT FOYER\n",
    "        90\tDUPLEX - ALL STYLES AND AGES\n",
    "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "       150\t1-1/2 STORY PUD - ALL AGES\n",
    "       160\t2-STORY PUD - 1946 & NEWER\n",
    "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, month sold is more of a categorical feature than a numeric feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"MoSold\"].unique() # Month Sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ID often doesn't help\n",
    "drop_features = [\"Id\"]\n",
    "\n",
    "# Looking at all columns, these are the actual numeric features\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\", \n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "I've not looked at all the features carefully. It might be appropriate to apply some other encoding on some of the numeric features above. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "set(numeric_looking_columns) - set(numeric_features) - set(drop_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll treat the above numeric-looking features as **categorical features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There are a bunch of ordinal features in this dataset. \n",
    "- Ordinal features with the same scale \n",
    "    - Poor (Po), Fair (Fa), Typical (TA), Good (Gd), Excellent (Ex)\n",
    "    - These we'll be calling `ordinal_features_reg`.\n",
    "- Ordinal features with different scales\n",
    "    - These we'll be calling `ordinal_features_oth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass the above as categories in our `OrdinalEncoder`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There are a bunch more ordinal features using different scales.\n",
    "  - These we'll be calling `ordinal_features_oth`. \n",
    "  - We are encoding them separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    ['NA', 'No', 'Mn', 'Av', 'Gd'],\n",
    "    ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],    \n",
    "    ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The remaining features are categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)    \n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We could also **engineer custom features**. \n",
    "  - e.g. **price per square foot** instead of **price** is decent feature engineering given our expert knowledge of housing market ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying feature transformations\n",
    "\n",
    "- Since we have mixed feature types, let's use `ColumnTransformer` (using the `make_column_transformer` interface) to apply different transformations on different features types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Create 4 pipelines, each \n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),    \n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examining the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train) # Calling fit to examine all the transformers.\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "new_columns = numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create new DataFrame by transforming the original DataFrame with the `preprocessor`\n",
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), \n",
    "    index=X_train.index, \n",
    "    columns=new_columns\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 80 features to 263 features!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other possible preprocessing?  \n",
    "\n",
    "- There is a lot of room for improvement.\n",
    "- We're just using `SimpleImputer`.\n",
    "    - In reality we'd want to go through this more carefully.\n",
    "    - We may also want to drop some columns that are almost entirely missing.    \n",
    "- We could also check for **outliers**, and do other exploratory data analysis (EDA).\n",
    "- But for now this is good enough ...    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply `Ridge` Regression\n",
    "\n",
    "- Recall that we are going to use `Ridge()` instead of `LinearRegression()` in this course. \n",
    "    - It has a hyperparameter `alpha` which controls **regularization** and, in turn, controls the the **fundamental bias/variance tradeoff**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, Ridge(alpha=1.0))\n",
    "pd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quite a bit of **variance** in the test scores. \n",
    "- Performing **very poorly** in fold 8. Not sure why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning `alpha` hyperparameter of `Ridge`\n",
    "- Recall that `Ridge` has a hyperparameter `alpha` that controls the fundamental tradeoff.\n",
    "- This is like `C` in `LogisticRegression` but, annoyingly, `alpha` is the inverse of `C`.\n",
    "- That is, large `C` is like small `alpha` and vice versa.\n",
    "- Smaller `alpha`: lower training error (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10.0 ** np.arange(-5, 4, 1)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "for alpha in alphas:\n",
    "    lr = make_pipeline(preprocessor, Ridge(alpha=alpha))\n",
    "    results = cross_validate(lr, X_train, y_train, return_train_score=True)\n",
    "    train_scores.append(np.mean(results[\"train_score\"]))\n",
    "    cv_scores.append(np.mean(results[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, train_scores, label=\"train\")\n",
    "plt.semilogx(alphas, cv_scores, label=\"cv\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmax(cv_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems alpha=100 is the best choice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- **Larger `alpha`** leads to **smaller coefficients**. (Why?)\n",
    "- Smaller coefficients mean the predictions are less sensitive to changes in the data.\n",
    "- Hence less chance of overfitting (seeing big dependencies when you shouldn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "**[Optionl / Bounus] + Reminder**\n",
    "\n",
    "Larger alpha leads to smaller coefficients... here is why:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**Objective Function of Ridge Regression:**  \n",
    "  \n",
    "$$J(w) = ||y - Xw||^2_2 + alpha * ||w||^2_2$$\n",
    "\n",
    "So, as you make the hyperparameter **`alpha` bigger**, $||w||^2_2$ will get smaller to make **$J(w)$ smaller**.\n",
    "\n",
    "**[end of Optionl / Bounus]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `RidgeCV`\n",
    "\n",
    "BTW, because it's so common to want to tune `alpha` with `Ridge`, sklearn provides a class called `RidgeCV`, which automatically tunes `alpha` based on cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- New Code (but nothing new, just mixing RidgeRgression with HParam tuning for Alpha) ------------\n",
    "ridgecv_pipe = make_pipeline(preprocessor, RidgeCV(alphas=alphas, cv=10))\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "ridgecv_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = ridgecv_pipe.named_steps['ridgecv'].alpha_\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some predictions on the `X_test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned = make_pipeline(preprocessor, Ridge(alpha=best_alpha))\n",
    "lr_tuned.fit(X_train, y_train)\n",
    "lr_preds = lr_tuned.predict(X_test)\n",
    "lr_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds.max(), lr_preds.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examine the coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get the feature names of the transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "new_columns = numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"features\": new_columns,\n",
    "        \"coefficients\": lr_tuned.named_steps[\"ridge\"].coef_,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "Print features and their coefficients, sorted from biggest to smallest coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(\"coefficients\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to this model:\n",
    "\n",
    "- As `OverallQual` feature gets bigger the housing price will get bigger.\n",
    "- Presence of `Neighborhood_Edwards` will result in smaller house value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Metrics (score functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train.values[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tuned.predict(X_train)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **How good** are the predictions on average across all samples?\n",
    "- What's the **performance** of the model?\n",
    "\n",
    "The above two questions are equivalent.\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A number of popular scoring functions for regression:\n",
    "\n",
    "- mean squared error (MSE)\n",
    "- $R^2$\n",
    "- root mean squared error (RMSE)\n",
    "- mean absolute persentage error (MAPE)\n",
    "- symmetric mean absolute persentage error (sMAPE) [**optional**]\n",
    "\n",
    "See [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean squared error (MSE)\n",
    "\n",
    "A common metric is mean squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(y, \\hat{y}) = \\frac{1}{n}  \\sum_{i=1}^n (y_i - \\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = lr_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.mean((y_test_pred - y_test) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Perfect predictions: MSE = 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is also implemented in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ------------ new function -------------\n",
    "mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- MSE looks huge and unreasonable. There is an error of ~\\$1 Billion!\n",
    "- Is this score good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In regression **our target has units**. \n",
    "  - The target is in **dollars**, the mean squared error is in **$dollars^2$** \n",
    "- The score also depends on the **scale** of the targets (imagine cents vs. dollars). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Root mean squared error or RMSE\n",
    "\n",
    "- Root of MSE :)\n",
    "- **Perfect predictions: MSE = 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RMSE(y, \\hat{y}) = \\sqrt{\\frac{1}{n}  \\sum_{i=1}^n (y_i - \\hat{y_i})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The **unit** of **MSE** is in $dollars^2$.\n",
    "- The **unit** of **RMSE** is $dollars$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error of \\$28,000 makes more sense.\n",
    "<br><br><br><br><br>\n",
    "Can we dig deeper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test_pred, alpha=0.3)\n",
    "grid = np.linspace(y_train.min(), y_train.max(), 1000)\n",
    "plt.plot(grid, grid, \"--k\")\n",
    "plt.xlabel(\"true price\")\n",
    "plt.ylabel(\"predicte price\");\n",
    "plt.title(\"Test Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Here we can see a few cases where our prediction is way off.\n",
    "- Is there something weird about those houses, perhaps? **Outliers**? \n",
    "- Under the line means we're under-prediction, over the line means we're over-predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $R^2$\n",
    "\n",
    "A common score is the $R^2$\n",
    "\n",
    "- This is the score that `sklearn` uses by default when you call `score()` in regression:\n",
    "- It is easy to compare performance of models with $R^2$\n",
    "  - Example:\n",
    "    - $R^2(model_A)=0.4$\n",
    "    - $R^2(model_B)=0.8$\n",
    "    - **model B is twice as good as model A**\n",
    "- It represents the proportion of \"variance of y\" that has been **explained** by the \"independent variables\" in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(y, \\hat{y}) = \\frac{1}{n}  \\sum_{i=1}^n (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "$$R^2(y, \\hat{y}) = \\frac{\\text{SSR}}{\\text{SST}}  = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n",
    "\n",
    "SSR: Sum of Square of Residuals  \n",
    "SST: Sum of Square of Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Key points:\n",
    "- **Perfect Predictions: $R^2 = 1$**\n",
    "- **Negative values are very bad: \"worse than DummyRegressor\" (very bad)**\n",
    "- Question for you: What's the $R^2$ score of dummy model which always predicts the **mean**? (assume score on the train set)\n",
    "  - Answer: ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Warning**: MSE is **commutative** but $R^2$ is not **commutative**\n",
    "\n",
    "<br><br><br>\n",
    "Wait a minute... what was **commutative**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_test, y_test_pred))\n",
    "print(r2_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Read more about R2](https://en.wikipedia.org/wiki/Coefficient_of_determination) if interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "We got an RMSE of ~$30,000 before. \n",
    "- Question: Is \\$30,000 good with respect to the scale of the target data? (imagine Vancouver vs. Regina, Saskatchewan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- For a house worth \\$600k, it seems reasonable! That's 5% error.\n",
    "- For a house worth \\$60k, that is terrible. It's 50% error.\n",
    "\n",
    "We have both of these cases in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Let's talk **percentages**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about looking at percent error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "percent_errors = (y_test_pred - y_test) / y_test * 100.0\n",
    "percent_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are both positive (predict too high) and negative (predict too low)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can look at the absolute percent error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.abs(percent_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And, like MSE, we can take the average over examples. This is called mean absolute percent error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mape(true, pred):\n",
    "    return np.mean(np.abs((pred - true) / true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "my_mape(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `sklearn` to calculate MAPE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, this is quite interpretable.\n",
    "- On average, we have around **10% error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric mean absolute percentage error (sMAPE)\n",
    "\n",
    "[Optional / Bonus]\n",
    "\n",
    "- When calculating percentage error, shall we put the \"actual\" in denominator or the \"prediction\"? Why?\n",
    "\n",
    "Learn here: https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transforming the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- In Linear Regression models, we often minimize Least Squares Error (or similar, depending on the solvers/optimizers used)\n",
    "- `.fit()` minimizes Least Squares Error, **not MAPE (percentage error)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When minimizing MSE, the expensive houses will dominate because they have the biggest error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------------\n",
    "Imagine you only have 2 samples in the test set:\n",
    "\n",
    "Model A\n",
    "\n",
    "- Example 1: Truth: \\$50k, Prediction: \\\\$100k\n",
    "- Example 2: Truth: \\$500k, Prediction: \\\\$550k\n",
    "- RMSE: $50k\n",
    "- MAPE: 45%\n",
    "\n",
    "\n",
    "Model B\n",
    "\n",
    "- Example 1: Truth: \\$50k, Prediction: \\\\$60k\n",
    "- Example 2: Truth: \\$500k, Prediction: \\\\$600k\n",
    "- RMSE: $71k\n",
    "- MAPE: 20%\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How can we get `.fit()` to think about MAPE?\n",
    "- A common practice which tends to work is **log transforming** the targets.\n",
    "- That is, transform $y\\rightarrow \\log(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.log10(y_train), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can incorporate this in our pipeline using `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# transformer for log transforming the target\n",
    "best_alpha = 100\n",
    "ttr = TransformedTargetRegressor(\n",
    "    Ridge(alpha=best_alpha), \n",
    "    func=np.log1p,  #  Calculates natural logarithm of log(1 + x)\n",
    "    inverse_func=np.expm1\n",
    ") \n",
    "ttr_pipe = make_pipeline(preprocessor, ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ttr_pipe.fit(X_train, y_train); # y_train automatically transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ttr_pipe.predict(X_train)  # predictions automatically un-transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "Now evaluating our model on Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = ttr_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We reduced MAPE from ~10% to ~8% with this trick! \n",
    "- We reduced RMSE from 28k to 22k with this trick! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----------\n",
    "**[Reminder from last session]**  \n",
    "**[Study on your own]**\n",
    "\n",
    "# Different scoring functions with `cross_validate`\n",
    "\n",
    "- Let's try using MSE instead of the default $R^2$ score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    cross_validate(\n",
    "        lr_tuned,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        return_train_score=True,\n",
    "        scoring=[\"neg_mean_squared_error\", \"neg_mean_absolute_percentage_error\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mape(true, pred):\n",
    "    return mean_absolute_percentage_error(true, pred)\n",
    "\n",
    "# make a scorer function that we can pass into cross-validation\n",
    "mape_scorer = make_scorer(mape, greater_is_better=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cross_validate(\n",
    "        lr_tuned, X_train, y_train, return_train_score=True, scoring=mape_scorer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    \"r2\": \"r2\",\n",
    "    \"neg_mape\": \"neg_mean_absolute_percentage_error\", \n",
    "    \"neg_rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mse\": \"neg_mean_squared_error\",\n",
    "}\n",
    "\n",
    "pd.DataFrame(\n",
    "    cross_validate(lr_tuned, X_train, y_train, return_train_score=True, scoring=scoring)\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, lr_tuned.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[End of Study on your own]**\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using regression metrics with `scikit-learn`\n",
    "\n",
    "- In `sklearn` you will notice that it has negative version of the metrics above (e.g., `neg_mean_squared_error`, `neg_root_mean_squared_error`). \n",
    "  - The reason for this is that scores return a value to **maximize**, the **higher the better**.\n",
    "- If you define your own scorer function and if you do not want this interpretation, you can set the `greater_is_better` parameter to False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (iClicker) Exercise 10.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) Outliers in the Training set adversely impact the performance of the model.\n",
    "- (B) The `alpha` hyperparameter of `Ridge` has similar interpretation of `C` hyperparameter of `LogisticRegression`; higher `alpha` means more complex model. \n",
    "- (C) In regression, one should use MAPE instead of MSE when relative (percentage) error matters more than absolute error.\n",
    "- (D) A lower RMSE value indicates a better model.\n",
    "- (E) We can still use precision and recall for regression problems but now we have other metrics we can use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- House prices dataset target is price, which is numeric -> regression rather than classification\n",
    "- There are corresponding versions of all the tools we used:\n",
    "    - `DummyClassifier` -> `DummyRegressor`\n",
    "    - `LogisticRegression` -> `Ridge`\n",
    "- `Ridge` hyperparameter `alpha` is like `LogisticRegression` hyperparameter `C`, but opposite meaning\n",
    "- We'll avoid `LinearRegression` in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Scoring metrics\n",
    "- $R^2$ is the default .score(), it is unitless, 0 is bad, 1 is best\n",
    "- MSE (mean squared error) is in units of target squared, hard to interpret; 0 is best\n",
    "- RMSE (root mean squared error) is in the same units as the target; 0 is best\n",
    "- MAPE (mean absolute percent error) is unitless; 0 is best, 1 is bad"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
