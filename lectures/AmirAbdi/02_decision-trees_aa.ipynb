{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 2: Terminology, Baselines, Decision Trees\n",
    "\n",
    "- UBC 2022-23, CPSC 330, Section 203\n",
    "- Instructor: Amir Abdi, call me **Amir**, **UBC** PhD Alumni, Principal Applied Scientist **Microsoft**, Co-founder/CTO startup\n",
    "\n",
    "**Contact:** Piazza + Office Hours (Mondays, 5-6 PM, virtual on Zoom)\n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📹 PreWatch and PostWatch:\n",
    "- Pre-watch: \n",
    "  - [2.1](https://youtu.be/YNT8n4cXu4A) \n",
    "  - [2.2](https://youtu.be/6eT5cLL-2Vc)\n",
    "- After lecture: \n",
    "  - [2.3](https://youtu.be/Hcf19Ij35rA)\n",
    "  - [2.4](https://youtu.be/KEtsfXn4w2E)                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements \n",
    "\n",
    "- Whether 330 can be a pre-requisite of 440: asking the department, have no update.\n",
    "- First deliverables\n",
    "  - First homework assignment is due this coming **Monday, Jan 16th, at 11:59pm**\n",
    "  - The assignment is available on **GitHub**\n",
    "  - You must do the first homework assignment **alone**\n",
    "  - The **Syllabus quiz** is available on [Canvas](https://canvas.ubc.ca/courses/106375/quizzes/584868) and is due **this coming Monday, Jan 16th, at 11:59pm.**\n",
    "- You can find the tentative due dates for all deliverables on the [course Github page](https://github.com/UBC-CS/cpsc330-2022W2). \n",
    "\n",
    "#### Note\n",
    "- Please monitor Piazza (especially pinned posts and instructor posts) for announcements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways from \"Why want to learn ML?\" from last session\n",
    "- Language Models, NLP, GPT/ChatGPT, Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes \n",
    "From this lecture, you will be able to \n",
    "\n",
    "Definitions\n",
    "\n",
    "- identify whether a given problem could be solved using **supervised** machine learning or not; \n",
    "- differentiate between supervised and **unsupervised machine** learning;\n",
    "- explain machine learning terminology such as **features, targets, predictions, training**, and **error**;\n",
    "- explain the difference between **parameters** and **hyperparameters**; \n",
    "- differentiate between **classification** and **regression** problems;\n",
    "\n",
    "\n",
    "ML Models\n",
    "- explain the **`fit`** and **`predict`** paradigm and use **`score`** method of ML models; \n",
    "\n",
    "Decision Trees\n",
    "- describe how **decision tree** prediction works;\n",
    "- use `DecisionTreeClassifier` and `DecisionTreeRegressor` to build decision trees using `scikit-learn`; \n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legends\n",
    "\n",
    "| <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/This_is_the_photo_of_Arthur_Samuel.jpg\" width=\"100\"> | <img src=\"http://www.cs.cmu.edu/~tom/TomHead2-6-22-22.jpg\" width=\"100\">  |\n",
    "| ----------- | ----------- |\n",
    "| Arthur Samuel       | Tom Mitchell       |\n",
    "| (1901-1990)    | 1951 - Now       |\n",
    "| First computer learning program | 1997 ML Texbook, CMU Prof |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology [[Pre-watch video](https://youtu.be/YNT8n4cXu4A)]\n",
    "\n",
    "You will see a lot of variable terminology in machine learning and statistics. Let's familiarize ourselves with some of the basic terminology used in ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Big picture and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll talk about our first machine learning model: **Decision trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Toy datasets \n",
    "Later in the course we will use larger datasets from Kaggle, for instance.  \n",
    "But for our first couple of lectures, we will be working with the following three toy datasets (all 3 added to the repo):  \n",
    "\n",
    "- [Quiz2 grade prediction classification dataset](../data/quiz2-grade-toy-classification_aa.csv)\n",
    "- [Quiz2 grade prediction regression dataset](../data/quiz2-grade-toy-regression.csv)\n",
    "- [Canada USA cities dataset](../data/canada_usa_cities.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "----\n",
    "If it's not necessary for you to understand the `code` package, it is designed to avoid clutter in this notebook  \n",
    "(e.g., most of the plotting code is going to be in `code/plotting_functions.py`)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Tabular data\n",
    "**tabular** format is a common modality of data, where \n",
    "- rows are **examples** and \n",
    "- columns are **features**. \n",
    "  - One of the columns is typically the **target**. \n",
    "\n",
    "<img src=\"../img/sup-ml-terminology.png\" height=\"1000\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Terminology:\n",
    "\n",
    "- **Features** \n",
    ": Features are relevant characteristics of the problem. Features are typically denoted by $X$ and the number of features is usually denoted by $d$.  \n",
    "\n",
    "- **Target**\n",
    ": Target is the feature we want to predict (typically denoted by $y$). \n",
    "\n",
    "- **Sample (Example)** \n",
    ": A row of feature values. The number of samples can be denoted by $n$. \n",
    "\n",
    "- **Training**\n",
    ": The process of learning the mapping between the features ($X$) and the target ($y$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Example: Tabular data for grade prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Learn Terminology via Example:\n",
    ">  Imagine that you are taking a course with four home work assignments and two quizzes. You and your friends are quite nervous about your quiz2 grades and you want to know how will you do based on your \n",
    "> - previous performance and \n",
    "> - some other attributes.   \n",
    "> \n",
    "> So you decide to collect some data from your friends from last year and train a supervised machine learning model for quiz2 grade prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../code/.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Some helper functions to save us time\n",
    "import sys\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(\"../data/quiz2-grade-toy-classification_aa.csv\")\n",
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "So the first step in training a supervised machine learning model is separating `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = classification_df.drop(columns=[\"quiz2\"])\n",
    "y = classification_df[\"quiz2\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Tabular data for the housing price prediction\n",
    "\n",
    "Here is an example of tabular data for housing price prediction. You can download the data from [here](https://www.kaggle.com/harlfoxem/housesalesprediction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"../data/kc_house_data.csv\")\n",
    "housing_df\n",
    "\n",
    "housing_df.drop([\"id\", \"date\"], axis=1, inplace=True)\n",
    "HTML(housing_df.head().to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = housing_df.drop(columns=[\"price\"])\n",
    "y = housing_df[\"price\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "Training data shapes the reality of models!  \n",
    "(Model's perception of world is shaped by the training data)\n",
    "<br><br><br><br><br><br>\n",
    "To a machine, column names (features) have no meaning.   \n",
    "Only feature values and how they vary across examples mean something. \n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative terminology for examples, features, targets, and training\n",
    "\n",
    "- **samples** = examples = rows = records = instances = *inputs*\n",
    "- **features** = predictors = regressors = independent variables = covariates = *inputs*\n",
    "- **targets** = outputs = outcomes = dependent variable = labels (if categorical)\n",
    "- **training** = learning = fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check out [the UBC Master of Data Science (MDS) terminology document](https://ubc-mds.github.io/resources_pages/terminology/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised learning vs. Unsupervised learning\n",
    "\n",
    "In **supervised learning**, \n",
    "- training data comprises a set of features ($X$) and their corresponding targets ($y$). \n",
    "- We wish to find a **model function $f$** that relates $X$ to $y$ \n",
    "  - given $X_{train}$ and $y_{train}$  :   $y_{train} = f(X_{train})$\n",
    "- Then use that model function **to predict the targets** of new examples. \n",
    "  - given $X_{test}$ and $f$  :   $y_{test} = f(X_{test})$\n",
    "\n",
    "\n",
    "<img src=\"../img/sup-learning.png\" height=\"900\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In **unsupervised learning** \n",
    "- training data consists of observations ($X$) **without any corresponding targets**. \n",
    "- Unsupervised learning could be used to (not an exhaustive list)\n",
    "  - **group (cluster) similar things together**, or\n",
    "  - summarize data, or\n",
    "  - learn patterns, or\n",
    "  - Learn association rules.  \n",
    "  \n",
    "\n",
    "\n",
    "<img src=\"../img/unsup-learning.png\" alt=\"\" height=\"900\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification and Regression \n",
    "Two **most common** supervised machine learning problems are:\n",
    "- **Classification problem**: predicting among two or more discrete classes\n",
    "    - Example1: Predict whether a patient has a liver disease or not &emsp;  (two classes -> binary classification)\n",
    "    - Example2: Predict whether a student would get A+ or not &emsp;&emsp; &emsp;(two classes -> binary classification)\n",
    "- **Regression problem**: predicting a continuous value\n",
    "    - Example1: Predict housing prices \n",
    "    - Example2: Predict a student's score in quiz2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../img/classification-vs-regression.png\" height=\"1500\" width=\"1500\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# quiz2 classification toy data\n",
    "classification_df = pd.read_csv(\"../data/quiz2-grade-toy-classification_aa.csv\")\n",
    "print(classification_df.shape)\n",
    "classification_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# quiz2 regression toy data\n",
    "regression_df = pd.read_csv(\"../data/quiz2-grade-toy-regression.csv\")\n",
    "print(regression_df.shape)\n",
    "regression_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "1. How many examples and features are there in the housing price data above? \n",
    "> You can use `df.shape` to get number of rows and columns in a dataframe. \n",
    "2. For each of the following examples what would be the relevant **features** and what would be the **target**?\n",
    "    1. Sentiment analysis\n",
    "    2. Financial Fraud Detection \n",
    "    3. Face recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iClicker Exercise 2.2 \n",
    "\n",
    "<img src=\"img_aa/iclicker_qr_code.png\" height=\"200\" width=\"200\"> \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Select all of the following statements which are examples of supervised machine learning**\n",
    "\n",
    "- (A) Finding groups of similar properties in a real estate data set.\n",
    "- (B) Predicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement. \n",
    "- (C) Grouping articles on different topics from different news sources (something like the Google News app). \n",
    "- (D) Detecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n",
    "- (E) Given some measure of employee performance, identify the key factors which are likely to influence their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iClicker Exercise 2.3 \n",
    "\n",
    "\n",
    "Select **all of the following statements** (multiple) which are examples of regression problems\n",
    "\n",
    "- (A) Predicting the price of a house based on features such as number of bedrooms and the year built.\n",
    "- (B) Predicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n",
    "- (C) Predicting percentage grade in CPSC 330 based on past grades.\n",
    "- (D) Predicting whether you should bicycle tomorrow or not based on the weather forecast.\n",
    "- (E) Predicting appropriate thermostat temperature based on the wind speed and the number of people in a room.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines [[video](https://youtu.be/6eT5cLL-2Vc)]\n",
    "\n",
    "- A baseline is a simple statistical or vanilla ML model you first try on the data\n",
    "- Baselines help you with sanity check of the problem\n",
    "- Baselines also give you some perspective on the \"gained performance\" of your best model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Example baseline:\n",
    "- Predict next grade of a student given their previous grades\n",
    "  - Baseline: average of student's previous grades\n",
    "- Predict whether the price of stock XYZ goes up (bull) or down (bear) next week?\n",
    "  - Baseline: if it went down last week, predict upwards trend, and vice versa.\n",
    "- Given a multi-choice exam, read all the questions and choices (Natural Language text) and answer correctly.\n",
    "  - Baseline: Random or always pick c (estimated expectation of 1/n, where n = numberOfChoices)\n",
    "  \n",
    "\n",
    "Examples of simple ML model baselines (depends on the problem):\n",
    "- Decision tree (will learn today!)\n",
    "- linear regression (lecture 7)\n",
    "- A multi-layer perceptron (learn end of march)\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees [[video](https://youtu.be/Hcf19Ij35rA)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Steps to train a classifier using `sklearn` \n",
    "\n",
    "1. Read the data\n",
    "2. Create $X$ and $y$\n",
    "3. Create a classifier object\n",
    "4. `fit` the classifier\n",
    "5. `predict` on new examples\n",
    "6. `score` the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Let's build a very simple supervised machine learning model for quiz2 grade prediction problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(\"../data/quiz2-grade-toy-classification_aa.csv\")\n",
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df['quiz2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create $X$ and $y$\n",
    "\n",
    "- $X$ &rarr; Feature vectors\n",
    "- $y$ &rarr; Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = classification_df.drop(columns=[\"quiz2\"])\n",
    "y = classification_df[\"quiz2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Writing a traditional program to predict quiz2 grade\n",
    "\n",
    "- Forget about ML for a second. If you are asked to write a program to predict whether a student gets an A+ or not in quiz2, how would you go for it?  \n",
    "- For simplicity, let's binarize the feature values. \n",
    "\n",
    "\n",
    "<img src=\"../img/quiz2-grade-toy.png\" height=\"700\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can we write a program with if/else?\n",
    "\n",
    "```\n",
    "    if class_attendance == 1 and quiz1 == 1:\n",
    "        quiz2 == \"A+\"\n",
    "    elif class_attendance == 1 and lab3 == 1 and lab4 == 1:\n",
    "        quiz2 == \"A+\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-------\n",
    "- How many possible rule combinations there could be with the given 7 binary features? \n",
    "    - Gets unwieldy pretty quickly \n",
    "\n",
    "\n",
    "- What if you had more features?\n",
    "    \n",
    "- What if you had 1M samples? (instead of only 20)\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Decision tree algorithm**: A machine learning algorithm to derive such rules from data in a principled way.  \n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building decision trees with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Let's build a very simple supervised machine learning model for quiz2 grade prediction problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "classification_df = pd.read_csv(\"../data/quiz2-grade-toy-classification_aa.csv\")\n",
    "\n",
    "classification_df = classification_df.rename(columns={\"quiz2\": \"target\"})\n",
    "X = classification_df.drop(columns=[\"target\"])\n",
    "y = classification_df[\"target\"]\n",
    "\n",
    "# columns = [\"lab1\", \"lab2\", \"lab3\", \"lab4\", \"quiz1\"]\n",
    "print(\"shape:\", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create a classifier object\n",
    "\n",
    "- `import` the appropriate classifier \n",
    "- Create an object of the classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier() # Create a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y) # Fit a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `score` (evaluate) the model\n",
    "  - How do you know how well your model is doing?\n",
    "  - For classification problems, by default, `score` gives the **accuracy** of the model, i.e., proportion of correctly predicted targets.  \n",
    "\n",
    "    $accuracy = \\frac{\\text{correct predictions}}{\\text{total examples}}$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y) # Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 100 accuracy... That's good news!  \n",
    "\n",
    "Let's visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some terminology related to trees \n",
    "\n",
    "Here is a commonly used terminology in a typical representation of decision trees. \n",
    "- **A root node**: represents the first condition to check or question to ask\n",
    "- **A branch**: connects a node (condition) to the next node (condition) in the tree. Each branch typically represents either true or false. \n",
    "- **A leaf node**: represents the predicted class/value when the path from root to the leaf node is followed. \n",
    "- **An internal node** : represents conditions within the tree\n",
    "- **Tree depth**: The number of edges on the path from the root node to the farthest away leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does `predict` work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "new_example = pd.DataFrame(data=[[0,1,0,89,80,77, 90]], columns=X.columns)\n",
    "new_example_pred = model.predict(new_example)\n",
    "\n",
    "print(new_example, '\\n')\n",
    "print(\"prediction:\", new_example_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In summary, given a learned tree and a test example, during prediction time,  \n",
    "- Start at the top of the tree. Ask binary questions at each node and follow the appropriate path in the tree. Once you are at a leaf node, you have the prediction. \n",
    "- Note that the model only considers the features which are in the learned tree and ignores all other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test a weird sample\n",
    "new_example = pd.DataFrame(data=[[0,0,0,0,100,0,0]], columns=X.columns)\n",
    "new_example_pred = model.predict(new_example)\n",
    "\n",
    "print(new_example, '\\n')\n",
    "print(\"prediction:\", new_example_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "Training data shapes the reality of models!  \n",
    "(Model's perception of world is shaped by the training data)\n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does `fit` work? (what were those `gini` values in the plot?)\n",
    "\n",
    "\n",
    "- Which features are most useful for classification? \n",
    "- Minimize **impurity** at each question\n",
    "- Common criteria to minimize impurity: [gini index (gini impurity)](https://scikit-learn.org/stable/modules/tree.html#classification-criteria), information gain, cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training: Tree each built from root down to the leaves\n",
    "- At each node, the algorithm should make 2 decisions:\n",
    "  - which **feature** to use\n",
    "  - what **threshold** to set to partition samples into left and right branches\n",
    "  \n",
    "Gini Impurity: \n",
    "- choose the **feature** and **threshold** that minimizes **Gini Impurity**\n",
    "\n",
    "Let's look at some samples:\n",
    "- t1=[A+, A+, A+, A+] &emsp; &emsp;&emsp; &emsp;--> Pure --> gini_impurity(t1) = 0\n",
    "- t2=[not A+, not A+, not A+] &emsp;--> pure --> gini_impurity(t2) = 0\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df[classification_df.lab3 <= 83.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Gini_{lab3}(D_{lab3 <= 83.5}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition of Gini Impurity**: A measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "$Gini(D) = 1 - \\sum_{i=1}^{C}p_i^{2}$\n",
    "\n",
    "$D$: set of data  \n",
    "$p_i$ = probability of samples belonging to class $i$ =  $ \\frac{N(class_i)}{N(total)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df[classification_df.lab3 > 83.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(A+) = 9 / 14$\n",
    "\n",
    "$p(Not A+) = 5 / 14$\n",
    "\n",
    "$Gini_{lab3}(D_{lab3 > 83.5}) = 1 - (9/14)^2 + (5/14)^2 = 1 - 0.413265306122449 + 0.127551020408163 = 0.459183673469388$  \n",
    "\n",
    "$Gini_{lab3} = 14/20 * 0.459183673469388 + 6/20 * 0 = 0.321428571428572$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Now, let's calculate the total Gini of making the above decision:    \n",
    "\n",
    "(WEIGHED SUM)\n",
    "\n",
    "$Gini_F = \\frac{n_1}{n} * Gini(D_1) + \\frac{n_2}{n} * Gini(D_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Gini_{lab3} = 0.459$\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "- What is Gini impurity of all leaves?  \n",
    "A) 0  \n",
    "B) 0.5  \n",
    "C) 1.0  \n",
    "D) Could have any value.  \n",
    "  \n",
    "- What was the Gini Impurity of the original data, before any partitioning?  \n",
    "A) 0  \n",
    "B) 0.495  \n",
    "C) 1.0  \n",
    "D) Can't be calculated\n",
    "\n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../img/eva-coffee.png)\n",
    "\n",
    "- We will try to take a 5-minute break half way through every class.\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision tree for regression problems\n",
    "\n",
    "- We can also use decision tree algorithm for regression. \n",
    "- Instead of gini, we use [some other criteria](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation) for splitting. A common one is mean squared error (MSE). (More on this in later videos.)\n",
    "- `scikit-learn` supports regression using decision trees with `DecisionTreeRegressor` \n",
    "    - `fit` and `predict` paradigms similar to classification\n",
    "    - `score` returns a value known as [$R^2$ score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score).     \n",
    "        - The maximum $R^2$ is 1 for perfect predictions. \n",
    "        - It can be negative which is very bad (worse than [DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)). \n",
    "\n",
    "What is [DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)?\n",
    "- Always predict the mean or median of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "regression_df = pd.read_csv(\"../data/quiz2-grade-toy-regression.csv\")\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = regression_df.drop([\"quiz2\"], axis=1)\n",
    "y = regression_df[\"quiz2\"]\n",
    "\n",
    "reg_model = tree.DecisionTreeRegressor()\n",
    "reg_model.fit(X, y); \n",
    "\n",
    "regression_df[\"predicted_quiz2\"] = reg_model.predict(X)\n",
    "print(\"R^2 score on the training data: %0.3f\\n\\n\" % (reg_model.score(X, y)))\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(reg_model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iClicker Exercise 2.5 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE (multi-answer).**\n",
    "\n",
    "- (A) If a baseline regressor always predicts the mean of the data (e.g `DummyRegressor`), change in features would change its predictions. \n",
    "- (B) `predict` takes only `X` as argument whereas `fit` and `score` take both `X` and `y` as arguments. \n",
    "- (C) For the decision tree algorithm to work, the feature values must be binary.\n",
    "- (D) The prediction in a decision tree works by routing the example from the root to the leaf.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More terminology [[video](https://youtu.be/KEtsfXn4w2E)]\n",
    "\n",
    "- Parameters and hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters \n",
    "\n",
    "- The decision tree algorithm primarily learns two things: \n",
    "    - the best feature to split on\n",
    "    - the threshold for the feature to split on at each node\n",
    "- These are called **parameters** of the decision tree model.  \n",
    "- When predicting on new examples, we need parameters of the model. \n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "**Parameters** are learned during the **training** process.\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping track of scores\n",
    "scores = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(\"../data/quiz2-grade-toy-classification_aa.csv\")\n",
    "X = classification_df.drop(columns=[\"quiz2\"])\n",
    "y = classification_df[\"quiz2\"]\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y);\n",
    "\n",
    "# how good is this model on the training data?\n",
    "score = model.score(X, y)\n",
    "print('performance on training data:', score)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- With the default setting, the nodes are expanded until all leaves are \"pure\" (Zero Gini Impurity). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The decision tree is creating very specific rules, based on just one example from the data. \n",
    "- Is it possible to control the learning in any way? \n",
    "    - Yes! One way to do it is by controlling the **depth** of the tree, which is the length of the longest path from the tree root to a leaf.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision tree with `max_depth=1`\n",
    "\n",
    "**Decision stump**\n",
    ": A decision tree with only one split (depth=1) is called a **decision stump**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X, y)\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`max_depth` is a **hyperparameter** of `DecisionTreeClassifier`. \n",
    "\n",
    "But, how good is this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how good is this model on the training data?\n",
    "score = model.score(X, y)\n",
    "print('performance on training data:', score)\n",
    "scores.insert(0, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "**Hyper-parameters (HParams)** are set by us!\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision tree with `max_depth=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    max_depth=3\n",
    ")  # Let's try another value for the hyperparameter\n",
    "model.fit(X, y)\n",
    "tree.plot_tree(model, feature_names=X.columns, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how good is this model on the training data?\n",
    "score = model.score(X, y)\n",
    "print('performance on training data:', score)\n",
    "scores.insert(1, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Let's plot the scores we just calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['', 'one','', 'three','', 'six'])\n",
    "ax.set_xlabel('depth')\n",
    "ax.set_ylabel('score on training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "In `sklearn` hyperparameters are set in the constructor. \n",
    "### Exercise 2.6: Is `depth` the only hyper-parameter of a Decision Tree?\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "**Parameters** are learned during the **training** process **to optimize a given criterion** (e.g. minimize Gini Impurity)\n",
    "\n",
    "**Hyper-parameters** are set by us based on:\n",
    "- expert knowledge\n",
    "- heuristic\n",
    "- systematic/automated optimization \n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above we looked at the `max_depth` hyperparameter. Some other commonly used hyperparameters of decision tree are:\n",
    "\n",
    "- `criterion`\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_leaf_nodes`\n",
    "- `class_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection\n",
    "\n",
    "What did we learn today? \n",
    "\n",
    "- There is a lot of terminology and jargon used in ML:\n",
    "    - Features, target, examples, training\n",
    "    - Supervised vs. Unsupervised machine learning     \n",
    "    - Classification\n",
    "    - Regression    \n",
    "    - Accuracy and error    \n",
    "    - Parameters and hyperparameters\n",
    "    - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees    \n",
    "  - learn a hierarchy of if/else questions\n",
    "  - make predictions by sequentially looking at features (from root to leaf) and checking if they pass a given condition\n",
    "  - One way to control the complexity of decision tree models is by using the depth hyper-parameter (`max_depth` in `sklearn`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Write your reflections (takeaways, struggle points, and general comments) on this material in [the reflection Google Document](https://docs.google.com/document/d/1nxULx7x1vypTLfSbXGH8ONJQAYxk2IoSRKdb4iltixY/edit?usp=sharing) \n",
    "\n",
    "- This is a collaborative GoogleDoc where you all have edit access.\n",
    "- I'll try to address those points in the next lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/eva-logging-off.png)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
