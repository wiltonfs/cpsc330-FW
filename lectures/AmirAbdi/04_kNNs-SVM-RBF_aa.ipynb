{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lecture 4: $k$-Nearest Neighbours and SVM RBFs  \n",
    "------------\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi\n",
    "\n",
    "iclicker link: https://join.iclicker.com/EMMJ <img src=\"img_aa/iclicker_qr_code.png\" height=\"300\" width=\"300\"> \n",
    "\n",
    "<!--\n",
    "replace (img/ wih (../img/  \n",
    "replace https://join.iclicker.com/3DP5H with https://join.iclicker.com/EMMJ\n",
    "replace \"data/ with \"../data\n",
    "copy legends\n",
    "copy setup cell\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- hw2 deadline is Jan 23, 11:59pm\n",
    "- [Last day to withdraw without a W standing](https://students.ubc.ca/enrolment/dates-deadlines#2022-23-winter-session-term-2): Jan 23, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain the notion of **similarity-based algorithms**; \n",
    "- broadly describe how **$k$-NN** use distances; \n",
    "- discuss the effect of using a small/large value of the hyperparameter $k$ when using the $k$-NN algorithm; \n",
    "- describe the problem of **curse of dimensionality**; \n",
    "- explain the general idea of **SVMs** and **RBF kernel**;\n",
    "  - broadly describe the relation of `gamma` and `C` hyperparameters of SVMs with the fundamental tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quick recap\n",
    "\n",
    "After last session you should know the following; if you don't go back and re-read the content:\n",
    "- **Train/Valid/Test** datasets and how they help you make better predictions on Prod data.\n",
    "- **Over-fitting** vs. **Under-fitting**\n",
    "- The **Bias** vs. **Variance** Tradeoff\n",
    "- **Golden rule of ML** (not look at test data too often)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1240/1*feFntGUIiob7MwUX62jdCg.webp\" height=\"500\" width=\"500\"> \n",
    "\n",
    "Source: https://prvnk10.medium.com/bias-variance-tradeoff-ebf13adcea42\n",
    "\n",
    "\n",
    "Total Error of the model = $Bias_{error} + Variance_{error} + \\sigma$ \n",
    "- **Bias** of the learning method: error of simplifying assumptions built into the method (e.g. DecisionTree is a simplifying assumption)\n",
    "- **Variance** of the learning method: How much the learning method will move around its mean **as the data changes**\n",
    "- **Irreducible error**: When `X` doesn't fully determine `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legends\n",
    "\n",
    "\n",
    "A simple method that everyone seems to have overlooked is to enclose the table within a div and use the align=\"center\" property on it.\n",
    "\n",
    "    \n",
    "| <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/This_is_the_photo_of_Arthur_Samuel.jpg\" width=\"100\"> | <img src=\"http://www.cs.cmu.edu/~tom/TomHead2-6-22-22.jpg\" width=\"100\">  | <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/49/John_McCarthy_Stanford.jpg\" width=\"100\"> |\n",
    "| :-----------: | :-----------: | :-----------: |\n",
    "| Arthur Samuel       | Tom Mitchell       |John McCarthy|\n",
    "| (1901-1990)    | 1951 - Now       |  1927 – 2011 |\n",
    "| First computer learning program | 1997 ML Texbook, CMU Prof | Co-coined term AI, Lisp,<br> Time-sharing, Garbage collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---------------\n",
    "If you want to run this notebook you will have to install `ipywidgets`. \n",
    "Follow the installation instructions [here](https://ipywidgets.readthedocs.io/en/latest/user_install.html).\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# install ipywidgets if not already installed\n",
    "!pip install ipywidgets\n",
    "\n",
    "# Some helper functions to save us time\n",
    "import sys\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mglearn\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation and distances [[video](https://youtu.be/hCa3EXEUmQk)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similarity (Analogy)-based models\n",
    "\n",
    "- Suppose you are given the following training examples with corresponding labels and are asked to label a given test example.\n",
    "\n",
    "![](../img/knn-motivation.png)\n",
    "<!-- <img src='./img/knn-motivation.png' width=\"1000\"> -->\n",
    "\n",
    "source of image: https://vipl.ict.ac.cn/en/database.php\n",
    "\n",
    "- An intuitive way to classify the test example is by finding the most \"similar\" example(s) from the training set and using that label for the test example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analogy-based algorithms in practice\n",
    "\n",
    "- High-tech Facial Recognition\n",
    "    - Feature vectors for human faces \n",
    "    - $k$-NN to identify which face is on their watch list\n",
    "- Recommendation systems     \n",
    "- Modern Machine Learning: Embeddings of a Large Language model (e.g. GPT) to find similar documents to a given document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea of $k$-nearest neighbours algorithm\n",
    "\n",
    "- Consider the following toy dataset with two classes.\n",
    "    - blue circles $\\rightarrow$ class 0\n",
    "    - red triangles $\\rightarrow$ class 1 \n",
    "    - green stars $\\rightarrow$ test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_forge()\n",
    "X_test = np.array([[8.2, 3.66214339], [9.9, 3.2], [11.2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions import plot_train_test_points\n",
    "plot_train_test_points(X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Given a new data point, predict the class of the data point by finding the \"**closest**\" data point in the training set,   \n",
    "  - Decide the class based on similarity by finding its \"**nearest neighbour**\" or \n",
    "  - **majority vote** of nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_knn_clf\n",
    "def f(n_neighbors):\n",
    "    return plot_knn_clf(X, y, X_test, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "interactive(\n",
    "    f,\n",
    "    n_neighbors=widgets.IntSlider(min=1, max=10, step=2, value=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Geometric view of features (What is a Dimension?)\n",
    "\n",
    "- To understand similarity (analogy)-based algorithms it's useful to think of data as points in a high dimensional space. \n",
    "- Our `X` represents the problem in terms of relevant **features** ($d$) with one dimension for each **feature** (column).\n",
    "- Examples are **points in a $d$-dimensional space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How many dimensions (features) are there in the cities data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"../data/canada_usa_cities.csv\")\n",
    "X_cities = cities_df[[\"longitude\", \"latitude\"]]\n",
    "y_cities = cities_df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the [Spotify Song Attributes](https://www.kaggle.com/geomack/spotifyclassification/home), how many dimensions does it have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"../data/spotify.csv\", index_col=0)\n",
    "X_spotify = spotify_df.drop(columns=[\"target\", \"song_title\", \"artist\"])\n",
    "X_spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of features in the Spotify dataset: %d\" % X_spotify.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [mnist dataset](https://en.wikipedia.org/wiki/MNIST_database#cite_note-1) is an important (but basic) computer vision dataset\n",
    "- Characters 0 to 9\n",
    "- You can develop a solution for MNIST in 5 minutes with 99% accuracy :)\n",
    "- Each image is **128 x 128** pixels\n",
    "- **Regression** or **Classification** problem?\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/tfds-data/visualization/fig/mnist-3.0.1.png\" width=\"300\"> <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"500\">\n",
    "\n",
    "\n",
    "**Question for you:**\n",
    "Number of features in MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Dimensions in ML problems \n",
    "\n",
    "Very naively put (**don't quote me on this**):\n",
    "\n",
    "- $d \\approx 20$ is considered low dimensional\n",
    "- $d \\approx 1000$ is considered medium dimensional \n",
    "- $d \\approx 100,000$ is considered high dimensional \n",
    "  - If you have an image input of `1024 x 768`, that's a 800k dimension feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature vectors \n",
    "\n",
    "**Feature vector**\n",
    ": is composed of feature values associated with an example.\n",
    "\n",
    "Some example feature vectors are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"An example feature vector from the Spotify dataset: \\n\\n%s\"\n",
    "    % (X_spotify.iloc[0].to_numpy())\n",
    ")\n",
    "\n",
    "X_spotify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Similarity between examples\n",
    "\n",
    "Let's take 2 points (two feature vectors) from the cities dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "two_cities = X_cities.sample(2, random_state=120)\n",
    "two_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The two sampled points are shown as big black circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(\n",
    "    X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities, s=8, alpha=0.3\n",
    ")\n",
    "mglearn.discrete_scatter(\n",
    "    two_cities.iloc[:, 0], two_cities.iloc[:, 1], markers=\"o\", c=\"k\", s=18\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Similarity Metrics and Similarity Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Euclidean distance \n",
    "\n",
    "$distance(u, v) = \\sqrt{\\sum_{i =1}^{n} (u_i - v_i)^2}$  \n",
    "$u = <u_1, u_2, \\dots, u_n>$\n",
    "$v = <v_1, v_2, \\dots, v_n>$\n",
    "\n",
    "where\n",
    "- $u$, $v$ are feature vectors of two samples\n",
    "- $n$ is the dimensionality of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cities, **Euclidean Distance** makes a lot of sense.\n",
    "\n",
    "For the cities at the two big circles, what is the _distance_ between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "two_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Subtract the two cities\n",
    "- Square the difference\n",
    "- Sum them up \n",
    "- Take the square root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the two cities\n",
    "print(\"Subtract the cities: \\n%s\\n\" % list((two_cities.iloc[1] - two_cities.iloc[0])))\n",
    "\n",
    "# Squared sum of the difference\n",
    "print(\n",
    "    \"Sum of squares: %0.4f\" % (np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2))\n",
    ")\n",
    "\n",
    "# Take the square root\n",
    "print(\n",
    "    \"Euclidean distance between cities: %0.4f\"\n",
    "    % (np.sqrt(np.sum((two_cities.iloc[1] - two_cities.iloc[0]) ** 2)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Euclidean distance using sklearn\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "euclidean_distances(two_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.pairwise_distances(two_cities, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise_distances(two_cities, metric='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "### Other Similarity Metrics\n",
    "Note: `scikit-learn` supports a number of other [distance metrics](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n",
    "\n",
    "In the order I expect you to prioritize learning them:\n",
    "- euclidean (l2)\n",
    "- manhattan (l1) (cityblock) (sum of absolute distances: ${\\sum_{i =1}^{n}|u_i - v_i|}$  \n",
    "- **cosine**\n",
    "- mahalanobis\n",
    "- haversine (angular distance between two points on a surface of a sphere)\n",
    "- and a few more (chebyshev, minkowski, wminkowski, seuclidean, ...)\n",
    "\n",
    "Check scikit-learn to learn more:\n",
    "-  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the nearest neighbour of City 0\n",
    "\n",
    "- Let's look at distances from all cities to all other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dists = euclidean_distances(X_cities)\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(dists, np.inf)\n",
    "pd.DataFrame(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the distances between **City 0** and some other cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which city is the closest to **City 0** based on the **Euclidean Distance**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector\n",
    "# min\n",
    "# argmin\n",
    "# ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following if to learn more details\n",
    "# print(\"Feature vector for city 0: \\n%s\\n\" % (X_cities.iloc[0]))\n",
    "# print(\"Distances from city 0 to the first 5 cities: %s\" % (dists[0][:5]))\n",
    "\n",
    "# We can find the closest city with `np.argmin`:\n",
    "print(\n",
    "    \"The closest city from city 0 is: %d \\n\\nwith feature vector: \\n%s\"\n",
    "    % (np.argmin(dists[0]), X_cities.iloc[np.argmin(dists[0])])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question**\n",
    "\n",
    "- Would we get the same results if we used a different metric (example, `l1` or `haversine`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics.pairwise_distances(X_cities, metric='l1')...\n",
    "# sklearn.metrics.pairwise_distances(X_cities, metric='haversine')..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the nearest neighbor of a query point (city)\n",
    "\n",
    "We can also find the distances to a new \"test\" or \"query\" city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find a city that's closest to the a query city\n",
    "query_point = [[-80, 25]]\n",
    "\n",
    "dists = euclidean_distances(X_cities, query_point)\n",
    "print(dists.shape)\n",
    "# dists[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query point is closest to\n",
    "print(\n",
    "    \"The query point %s is closest to the city with index %d and the distance between them is: %0.4f\"\n",
    "    % (query_point, np.argmin(dists), dists[np.argmin(dists)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-Nearest Neighbours ($k$-NNs) [[video](https://youtu.be/bENDqXKJLmg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evelyn Fix and Joseph Hodges are credited with the initial ideas around the KNN model [in this 1951 paper](https://apps.dtic.mil/sti/pdfs/ADA800276.pdf),  \n",
    "while Thomas Cover expands on their concept in [his research](https://isl.stanford.edu/~cover/papers/transIT/0021cove.pdf) “Nearest Neighbor Pattern Classification.”\n",
    "> Reference: https://www.ibm.com/topics/knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "small_cities = cities_df.sample(30, random_state=90)\n",
    "one_city = small_cities.sample(1, random_state=44)\n",
    "small_train_df = pd.concat([small_cities, one_city]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small_cities = small_train_df.drop(columns=[\"country\"]).to_numpy()\n",
    "y_small_cities = small_train_df[\"country\"].to_numpy()\n",
    "test_point = one_city[[\"longitude\", \"latitude\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_test_points(\n",
    "    X_small_cities,\n",
    "    y_small_cities,\n",
    "    test_point,\n",
    "    class_names=[\"Canada\", \"USA\"],\n",
    "    test_format=\"circle\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Given a new data point, predict the class of the data point by finding the \"closest\" data point in the training set, i.e., by finding its \"nearest neighbour\" or majority vote of nearest neighbours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we want to predict the class of the black point.  \n",
    "- An intuitive way to do this is predict the same label as the \"closest\" point ($k = 1$) (1-nearest neighbour)\n",
    "- We would predict a target of **USA** in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_clf(\n",
    "    X_small_cities,\n",
    "    y_small_cities,\n",
    "    test_point,\n",
    "    n_neighbors=1,\n",
    "    class_names=[\"Canada\", \"USA\"],\n",
    "    test_format=\"circle\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about using $k > 1$ to get a more robust estimate? \n",
    "- For example, we could also use the 3 closest points (*k* = 3) and let them **vote** on the correct class.  \n",
    "- The **Canada** class would win in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_clf(\n",
    "    X_small_cities,\n",
    "    y_small_cities,\n",
    "    test_point,\n",
    "    n_neighbors=3,\n",
    "    class_names=[\"Canada\", \"USA\"],\n",
    "    test_format=\"circle\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "for k in k_values:\n",
    "\n",
    "    # ---- New function ------\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_small_cities, y_small_cities)\n",
    "    # -----------------------\n",
    "    \n",
    "    print(\n",
    "        \"Prediction of the black dot with %d neighbours: %s\"\n",
    "        % (k, neigh.predict(test_point))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "In the KNN algorithm, `k` matters!\n",
    "\n",
    "is `k` a **hyper-parameter** or **parameter**?\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing `n_neighbors`\n",
    "\n",
    "- The primary hyperparameter of the model is `n_neighbors` ($k$) which decides how many neighbours should vote during prediction? \n",
    "- What happens when we play around with `n_neighbors`?\n",
    "- Are we more likely to **overfit** with a low `n_neighbors` or a high `n_neighbors`?\n",
    "- Let's examine the effect of the hyperparameter on our cities data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "X = cities_df.drop(columns=[\"country\"])\n",
    "y = cities_df[\"country\"]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.9, random_state=123\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, train_size=0.8, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "score = knn1.fit(X_train, y_train).score(X_valid, y_valid)\n",
    "print('Validation Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn100 = KNeighborsClassifier(n_neighbors=k)\n",
    "score = knn100.fit(X_train, y_train).score(X_valid, y_valid)\n",
    "print('Validation Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "knn100 = KNeighborsClassifier(n_neighbors=k)\n",
    "score = knn100.fit(X_train, y_train).score(X_valid, y_valid)\n",
    "print('Validation Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For students: Play around with this for fun!\n",
    "# ------------- To be skipped ----------------\n",
    "def f(n_neighbors=1):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score_valid = knn.score(X_valid, y_valid)\n",
    "    score_train = knn.score(X_train, y_train)\n",
    "    print('n_neighbors:', n_neighbors)\n",
    "    print('train score:', score_train)\n",
    "    print('valid score:', score_valid)\n",
    "\n",
    "interactive(\n",
    "    f,\n",
    "    n_neighbors=widgets.IntSlider(min=1, max=101, step=5, value=1),\n",
    ")\n",
    "# ------------- To be skipped ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from plotting_functions import plot_knn_decision_boundaries\n",
    "matplotlib.rc('font', **{'size'   : 10})\n",
    "\n",
    "plot_knn_decision_boundaries(X_train, y_train, k_values=[1, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_valid, scores_train = list(), list()\n",
    "n_neighbors_list = range(1, 50, 5)\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores_valid.append(knn.score(X_valid, y_valid))\n",
    "    scores_train.append(knn.score(X_train, y_train))\n",
    "\n",
    "results_df = pd.DataFrame({'train_accuracy': scores_train, 'valid_accuracy': scores_valid, 'n_neighbors': n_neighbors_list}).set_index('n_neighbors')\n",
    "results_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "**Question**:\n",
    "Why at `n_neighbors=1`, Train Accuracy is 100%?\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's the performance of the best model on test set?\n",
    "\n",
    "- `n_neighbors` is a hyperparameter\n",
    "- We can use hyperparameter optimization to choose `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you notice some weird pattern above? How do you explain that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------------------\n",
    "Now, we will evaluate our best model on **test data** to report to our stakeholders (customer, product manager, marketting, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "best_n_neighbours = results_df.idxmax()['valid_accuracy']\n",
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbours)\n",
    "\n",
    "# Let's use the entire Train + Valid dataset to train a new model\n",
    "knn.fit(pd.concat((X_train, X_valid)), pd.concat((y_train, y_valid)))\n",
    "\n",
    "print(\"Test accuracy: %0.3f\" % (knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iClicker) Exercise 4.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "1. Similarity (analogy)-based models find examples from the test set that are most similar to the query example we are predicting.\n",
    "2. We more likely to **overfit** with a low `n_neighbors` than a high `n_neighbors`\n",
    "3. With $k$-NN, setting the hyperparameter $k$ to larger values typically reduces training error. \n",
    "4. In $k$-NN, with $k > 1$, the classification of the closest neighbour to the test example always contributes the most to the prediction.\n",
    "5. KNN is such an old algorithm, oh dear! Why am I learning this? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../img/eva-coffee.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "```python\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "What's KNN learning when we call the `.fit()` function?\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lazy learning**: Takes no time to `fit`, i.e, **no fitting!** \n",
    "- There is no learning happening\n",
    "- Has no learning process\n",
    "- model = all the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compuational complexity of prediction on a new sample? \n",
    "\n",
    "- Have to **iterate** over all the sample one-by-one and calculate the **distance** to all, and get the **top k** samples.\n",
    "- Can be potentially be **VERY slow** during prediction time, especially when the training set is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Study on your own!]\n",
    "### More on $k$-NNs [[video](https://youtu.be/IRGbqi5S9gQ)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other useful arguments of `KNeighborsClassifier`\n",
    "\n",
    "- `weights` $\\rightarrow$ When predicting label, **you can assign higher weight to the examples which are closer to the query example**.  \n",
    "- **Exercise for you**: Play around with this argument. Do you get a better validation score? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression with $k$-nearest neighbours ($k$-NNs)\n",
    "\n",
    "- Can we solve regression problems with $k$-nearest neighbours algorithm? \n",
    "- In $k$-NN regression we take the average of the $k$-nearest neighbours. \n",
    "- We can also have **weighted regression**. \n",
    "\n",
    "See an example of regression in the lecture notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros of $k$-NNs for supervised learning\n",
    "\n",
    "- Easy to understand, interpret.\n",
    "- Simple hyperparameter $k$ (`n_neighbors`) controlling the fundamental tradeoff.\n",
    "- Can learn very complex functions given enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cons of $k$-NNs for supervised learning\n",
    "\n",
    "- Can be potentially be VERY slow during prediction time, especially when the training set is very large. \n",
    "- Often not that great test accuracy compared to the modern approaches.\n",
    "- It does not work well on datasets with many features or where most feature values are 0 most of the time (sparse datasets).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "For regular $k$-NN for supervised learning (not with sparse matrices), you should normalize (scale) your features.   \n",
    "We'll be looking into it soon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric vs Non-parametric Models\n",
    "\n",
    "- **Parametric Model:** Learning **finite number of learnable parameters** during training.\n",
    "  - Eventually, as we add more data, we get to a point where more data doesn’t help (too much bias, model is too simple).\n",
    "- **Non-parametric Model:** The number of **parameters can be potentially infinite**.\n",
    "\n",
    "A simplied explanation of **non-parametric model**: The model (what is stored with the model) **increases with the size of data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parametric example**: $k$-NN is a classic example of non-parametric models.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Are **DecisionTrees** parametric or non-parametric?\n",
    "\n",
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- If you want to know more about this, find some reading material \n",
    "  - https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L6.pdf\n",
    "  - http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf\n",
    "  - https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/\n",
    "- By the way, the terms \"parametric\" and \"non-paramteric\" are often used differently by statisticians, see [here](https://help.xlstat.com/s/article/what-is-the-difference-between-a-parametric-and-a-nonparametric-test?language=en_US) for more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "# Curse of dimensionality\n",
    "\n",
    "**When more features HARM rather than HELP**\n",
    "\n",
    "- Affects all learners but especially bad for similarity-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality on KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $k$-NN usually works well when the number of dimensions $d$ is small but things fall apart quickly as $d$ goes up.\n",
    "  - **Because $k$-NN cannot ignore any features**\n",
    "- If there are many irrelevant attributes, $k$-NN is hopelessly confused because all of them contribute to finding similarity between examples. \n",
    "- With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and $k$-NN is no better than random guessing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=30, n_features=4, n_classes=2)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.dummy import DummyClassifier\n",
    "```\n",
    "Randomly selects one of the two classes; it's expected accuracy is 50% on binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "nfeats_accuracy = {\"nfeats\": [], \"dummy_valid_accuracy\": [], \"KNN_valid_accuracy\": []}\n",
    "for n_feats in range(4, 2000, 100):\n",
    "    X, y = make_classification(n_samples=2000, n_features=n_feats, n_classes=2)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "    # DummyClassifier \n",
    "    dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "\n",
    "    # KNeighborsClassifier \n",
    "    knn = KNeighborsClassifier()\n",
    "    scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "    \n",
    "    nfeats_accuracy[\"nfeats\"].append(n_feats)\n",
    "    nfeats_accuracy[\"KNN_valid_accuracy\"].append(np.mean(scores[\"test_score\"]))\n",
    "    nfeats_accuracy[\"dummy_valid_accuracy\"].append(np.mean(dummy_scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nfeats_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines (SVMs) with RBF kernel [[video](https://youtu.be/ic_zqOhi020)]\n",
    "\n",
    "\n",
    "<img src=\"https://datascience.columbia.edu/wp-content/uploads/2020/08/Vapnik_web.png\" width=\"300\">\n",
    "\n",
    "Vladimir N. Vapnik, 1936 (age 86)\n",
    "\n",
    "> The original SVM algorithm was invented by **Vladimir N. Vapnik** and Alexey Ya. Chervonenkis** in 1963.   \n",
    "In 1992, **Bernhard Boser, Isabelle Guyon and Vladimir Vapnik** suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.\n",
    "\n",
    "- Very high-level overview\n",
    "- Our goals here are\n",
    "    - Use `scikit-learn`'s SVM model. \n",
    "    - Broadly explain the notion of support vectors.  \n",
    "    - Broadly explain the similarities and differences between $k$-NNs and SVM RBFs.\n",
    "    - Explain how `C` and `gamma` hyperparameters control the fundamental tradeoff.\n",
    "    \n",
    "> (Optional) RBF stands for radial basis functions. We won't go into what it means in this session. Refer to [this video](https://www.youtube.com/watch?v=Qc5IyLW_hns) if you want to know more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a Support Vector?\n",
    "\n",
    "Samples on the **boundaries** that determine the **margin hyperplane**\n",
    "\n",
    "\n",
    "A **hyperplane** can be defined as: \n",
    "\n",
    "${\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b=0}$\n",
    "\n",
    "$w$ and $b$ are learnable parameters, and learned during training of SVM\n",
    "<br><br><br><br><br>\n",
    "\n",
    "This is is a **Linear HARD SVM** \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1920px-SVM_margin.png\" width=\"500\">\n",
    "\n",
    "[img source](https://en.wikipedia.org/wiki/Support_vector_machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HARD**: No misclassification can happen  \n",
    "**SOFT**: Misclassification can happen, and each miscalssification will be penalized relative to `C`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this interactive demo: https://greitemann.dev/svm-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Another popular similarity-based algorithm is Support Vector Machines with RBF Kernel (SVM RBFs)\n",
    "- Superficially, SVM RBFs are more like weighted $k$-NNs.\n",
    "    - The decision boundary is defined by **a set of positive and negative examples** and **their weights** together with **their similarity measure**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The primary difference between $k$-NNs and SVM RBFs is that \n",
    "    - Unlike $k$-NNs, SVM RBFs only remember the key examples (**support vectors**). \n",
    "    - SVMs use a different similarity metric which is called a \"kernel\". \n",
    "      - A popular kernel is Radial Basis Functions (RBFs)\n",
    "    - They usually perform better than $k$-NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this interactive demo: https://greitemann.dev/svm-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's explore SVM RBFs\n",
    "\n",
    "Let's try SVMs on the cities dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cities, y_cities, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "An SVM used for **C**lassificatio is called **SVC**  \n",
    "Simiilarly,  \n",
    "An SVM used for **R**egression is called **SVR**  \n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# New Class: SVC!\n",
    "svm = SVC(gamma=0.01)  # Ignore gamma for now\n",
    "scores = cross_validate(svm, X_train, y_train, return_train_score=True)\n",
    "\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundary of SVMs \n",
    "- We can think of SVM with RBF kernel as \"smooth KNN\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "for clf, ax in zip([knn, svm], axes):\n",
    "    clf.fit(X_train, y_train)\n",
    "    mglearn.plots.plot_2d_separator(\n",
    "        clf, X_train.to_numpy(), fill=True, eps=0.5, ax=ax, alpha=0.4\n",
    "    )\n",
    "    mglearn.discrete_scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], y_train, ax=ax)\n",
    "    ax.set_title(clf)\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support vectors \n",
    "\n",
    "- Each training example either is or isn't a \"support vector\".\n",
    "  - This gets decided during `fit`.\n",
    "\n",
    "- **Main insight: the decision boundary only depends on the support vectors.**\n",
    "\n",
    "- Let's look at the support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n = 20\n",
    "n_classes = 2\n",
    "X_toy, y_toy = make_blobs(\n",
    "    n_samples=n, centers=n_classes, random_state=300\n",
    ")  # Let's generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_toy[:, 0], X_toy[:, 1], y_toy)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_toy[:, 0], X_toy[:, 1], y_toy)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, gamma=0.1).fit(X_toy, y_toy)\n",
    "mglearn.plots.plot_2d_separator(svm, X_toy, fill=True, eps=0.5, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_support_vectors\n",
    "plot_support_vectors(svm, X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vectors are the bigger points in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters of SVM \n",
    "\n",
    "- Key hyperparameters of **RBF (Radial Basis Function)** SVM are\n",
    "    - `gamma`: Defines **how far** the influence of a single training example reaches\n",
    "    - `C`: The C parameter **trades off correct classification** vs. **maximization of the decision margin**\n",
    "    \n",
    "- We are not equipped to understand the meaning of these parameters at this point but you are expected to describe their relation to the fundamental tradeoff. \n",
    "\n",
    "See [`scikit-learn`'s explanation of RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `gamma` and the fundamental trade-off\n",
    "\n",
    "- `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "  - larger `gamma` $\\rightarrow$ each point influences close, **more complex**\n",
    "  - smaller `gamma` $\\rightarrow$ each point influence far away, **less complex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_svc_gamma\n",
    "matplotlib.rc('font', **{'size'   : 10})\n",
    "\n",
    "gamma = [0.001, 0.01, 0.1, 1.0, 2.0]\n",
    "plot_svc_gamma(\n",
    "    gamma,\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy(),\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `C` and the fundamental trade-off\n",
    "\n",
    "- `C` _also_ affects the fundamental tradeoff\n",
    "- The bigger the `C` the more you **penalize misclssification**\n",
    "    - larger `C` $\\rightarrow$ penalize more, **more complex**, smaller decision margin\n",
    "    - smaller `C` $\\rightarrow$ penalize less, **less complex**, bigger decision margin\n",
    "    \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $nu$-SVM [Optional/Bonus Content]\n",
    "There is another version of SVM calld **$\\nu$-SVM** where `C` is replaced by `nu`\n",
    "- `nu` ranges from 0 to 1\n",
    "- `C` ranges from 0 to infinity\n",
    "- **Larger values of `nu`** correspond to **smaller values of `C`**\n",
    "- **Smaller values of `nu`** correspond to **larger values of `C`**\n",
    "- `C = infinity` $\\rightarrow$ `nu = 0` $\\rightarrow$ HARDer margin\n",
    "- `C = 0` $\\rightarrow$ `nu = 1` $\\rightarrow$ SOFTer margin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions import plot_svc_C\n",
    "matplotlib.rc('font', **{'size'   : 10})\n",
    "\n",
    "C = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "plot_svc_C(\n",
    "    C, X_train.to_numpy(), y_train.to_numpy(), x_label=\"longitude\", y_label=\"latitude\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM Regressor\n",
    "\n",
    "- Similar to KNNs, you can use SVMs for regression problems as well.\n",
    "- See [sklearn.svm.SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HParam Search over multiple hyperparameters\n",
    "What to do when you have 2+ HParam to decide?\n",
    "\n",
    "- In the above case the best training error is achieved by the most complex model (large `gamma`, large `C`).\n",
    "- Best validation error requires a hyperparameter search to balance the fundamental tradeoff.\n",
    "- How do you check all values of all Hyper-Parameters to decide the **best set of HParams?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More on this next week. But if you cannot wait till then, you may look up the following:\n",
    "    - [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "    - [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## (iClicker) Exercise 4.2 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "1. $k$-NN may perform poorly in high-dimensional space (say, *d* > 1000). \n",
    "2. In SVM RBF, removing a non-support vector would not change the decision boundary. \n",
    "3. In sklearn’s SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score. \n",
    "4. If we increase both gamma and C, we can't be certain if the model becomes more complex or less complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### More practice questions \n",
    "\n",
    "- Check out some more practice questions [here](https://ml-learn.mds.ubc.ca/en/module4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- We have **KNNs and SVMs as supervised learning** techniques in our toolbox.\n",
    "- These are similarity (analogy)-based learners and the idea is to assign nearby points the same label.\n",
    "- **Unlike decision trees, all features are equally important**\n",
    "- Both can be used for **classification** or **regression** (much like the other methods we've seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up:\n",
    "\n",
    "Lingering questions: \n",
    "- Are we ready to do machine learning on real-world datasets?\n",
    "- What would happen if we use $k$-NNs or SVM RBFs on the spotify dataset from hw1?  \n",
    "- What happens if we have missing values in our data? \n",
    "- What do we do if we have features with categories or string values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](../img/eva-seeyou.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
