{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lecture 5: SVM, Preprocessing and `sklearn` pipelines\n",
    "------------\n",
    "UBC 2022-23 W2\n",
    "\n",
    "Instructor: Amir Abdi\n",
    " - Mondays, 5-6 (or 5-7)\n",
    "\n",
    "iclicker link: https://join.iclicker.com/EMMJ <img src=\"img_aa/iclicker_qr_code.png\" height=\"300\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "- Homework 3 is out. Please start early. \n",
    "- Homework 1 solutions will be posted on Canvas. Please do not share them with anyone or do not post them anywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The lecture material is like your \"text book\"; I won't be covering every detail step-by-step; it's up to you to study and form questions + watch pre-read material**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- have another glimpse at **SVM**\n",
    "- explain motivation for **preprocessing** in supervised machine learning;\n",
    "- identify when to implement **feature transformations** such as **imputation, scaling, and one-hot encoding** in a machine learning model development pipeline; \n",
    "- use `sklearn` transformers for applying feature transformations on your dataset;\n",
    "- discuss golden rule in the context of feature transformations;\n",
    "- use `sklearn.pipeline.Pipeline` and `sklearn.pipeline.make_pipeline` to build a preliminary machine learning pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Classifiers and regressors\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "# Preprocessing and pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly learn **SVM**s, you need to learn about:\n",
    "- kernels (polynomial kernels, RBF kernel) --> not coverered here\n",
    "- support vectors\n",
    "- soft/hard SVMs\n",
    "- regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "\n",
    "## Kernel (kernel trick)\n",
    "- A method to use a linear-model to solve a non-linear problem.\n",
    "- A more efficient (less computationally expensive) way to transform data into higher dimensions\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4800/1*mCwnu5kXot6buL7jeIafqQ.webp\">\n",
    "\n",
    "\n",
    "Figure reference: https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "An SVM used for **C**lassificatio is called **SVC**  \n",
    "Simiilarly,  \n",
    "An SVM used for **R**egression is called **SVR**  \n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVC (SVM for classification) with RBF kernel\n",
    "\n",
    "Let's try SVMs on the cities dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mglearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_cities \u001b[38;5;241m=\u001b[39m cities_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      3\u001b[0m y_cities \u001b[38;5;241m=\u001b[39m cities_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmglearn\u001b[49m\u001b[38;5;241m.\u001b[39mdiscrete_scatter(X_cities\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m], X_cities\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m], y_cities)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mglearn' is not defined"
     ]
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"../data/canada_usa_cities.csv\")\n",
    "X_cities = cities_df[[\"longitude\", \"latitude\"]]\n",
    "y_cities = cities_df[\"country\"]\n",
    "\n",
    "mglearn.discrete_scatter(X_cities.iloc[:, 0], X_cities.iloc[:, 1], y_cities)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cities, y_cities, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# ------ New Class: SVC! ------\n",
    "svm = SVC(gamma=0.01)  # Ignore gamma for now\n",
    "svm.fit(X_train, y_train)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Test score %0.3f\" % test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision boundary of SVMs \n",
    "- We can think of SVM with RBF kernel as \"smooth KNN\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "svm = SVC(gamma=0.01)  # Ignore gamma for now\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "for clf, ax in zip([knn, svm], axes):\n",
    "    clf.fit(X_train, y_train)\n",
    "    mglearn.plots.plot_2d_separator(\n",
    "        clf, X_train.to_numpy(), fill=True, eps=0.5, ax=ax, alpha=0.4\n",
    "    )\n",
    "    mglearn.discrete_scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], y_train, ax=ax)\n",
    "    ax.set_title(clf)\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support vectors \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1920px-SVM_margin.png\" width=\"500\">\n",
    "\n",
    "[img source](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "- Each **training example** either is or isn't a \"support vector\".\n",
    "  - This gets decided during `fit`.\n",
    "\n",
    "- **Main insight: the decision boundary only depends on the support vectors.**\n",
    "\n",
    "- Let's look at the support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n = 20\n",
    "n_classes = 2\n",
    "X_toy, y_toy = make_blobs(\n",
    "    n_samples=n, centers=n_classes, random_state=300\n",
    ")  # Let's generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_toy[:, 0], X_toy[:, 1], y_toy)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_toy[:, 0], X_toy[:, 1], y_toy)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10, gamma=0.1).fit(X_toy, y_toy)\n",
    "mglearn.plots.plot_2d_separator(svm, X_toy, fill=True, eps=0.5, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_support_vectors\n",
    "plot_support_vectors(svm, X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vectors are the bigger points in the plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters of SVM \n",
    "\n",
    "- Important hyperparameter of **RBF (Radial Basis Function)**:\n",
    "  - `gamma`: Defines **how far** the influence of a single training example reaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Important hyperparameter of SVM:\n",
    "  - `C` (**regularization**): The C parameter **trades off correct classification on TRAIN** vs. **maximization of the decision margin**\n",
    "    - Larger `C` --> less regularization\n",
    "    - Smaller `C` --> more regularization (simpler function or simpler model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "We are not equipped to understand the meaning of these parameters at this point but you are expected to describe their relation to the fundamental tradeoff. \n",
    "\n",
    "See [`scikit-learn`'s explanation of RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive demo: (a very good demo!)**\n",
    "- https://dash.gallery/dash-svm/ (code is here: https://github.com/plotly/dash-sample-apps/tree/main/apps/dash-svm)\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `gamma` and the fundamental trade-off\n",
    "\n",
    "- `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "  - larger `gamma` $\\rightarrow$ each point influences close, **more complex**\n",
    "  - smaller `gamma` $\\rightarrow$ each point influence far away, **less complex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_svc_gamma\n",
    "matplotlib.rc('font', **{'size'   : 10})\n",
    "\n",
    "gamma = [0.001, 0.01, 0.1, 1.0, 2.0]\n",
    "plot_svc_gamma(\n",
    "    gamma,\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy(),\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relation of `C` and the fundamental trade-off\n",
    "\n",
    "- `C` _also_ affects the fundamental tradeoff\n",
    "- The bigger the `C` the more you **penalize misclssification**\n",
    "    - larger `C` $\\rightarrow$ penalize more, **more complex**, smaller decision margin\n",
    "    - smaller `C` $\\rightarrow$ penalize less, **less complex**, bigger decision margin\n",
    "    \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "[Not covered in class; for students to read out of **their never-ending curiosity to learn more**]\n",
    "### $nu$-SVM **[Optional/Bonus Content]**\n",
    "There is another version of SVM calld **$\\nu$-SVM** where `C` is replaced by `nu`\n",
    "- `nu` ranges from 0 to 1\n",
    "- `C` ranges from 0 to infinity\n",
    "- **Larger values of `nu`** correspond to **smaller values of `C`**\n",
    "- **Smaller values of `nu`** correspond to **larger values of `C`**\n",
    "- `C = infinity` $\\rightarrow$ `nu = 0` $\\rightarrow$ HARDer margin\n",
    "- `C = 0` $\\rightarrow$ `nu = 1` $\\rightarrow$ SOFTer margin\n",
    "- This demo is basd on $nu$-SVM\n",
    "  - https://greitemann.dev/svm-demo\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions import plot_svc_C\n",
    "matplotlib.rc('font', **{'size'   : 10})\n",
    "\n",
    "C = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "plot_svc_C(\n",
    "    C, X_train.to_numpy(), y_train.to_numpy(), x_label=\"longitude\", y_label=\"latitude\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM Regressor\n",
    "\n",
    "- Similar to KNNs, you can use SVMs for regression problems as well.\n",
    "- See [sklearn.svm.SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "## (iClicker) Exercise 4.2 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "1. $k$-NN may perform poorly in high-dimensional space (say, *d* > 1000). \n",
    "2. In SVM with RBF kernel, removing a non-support vector would not change the decision boundary. \n",
    "3. In Linear SVM, removing a non-support vector would not change the decision boundary.\n",
    "4. In sklearn’s SVC classifier, very large values of gamma tend to result in higher training score but probably lower validation score. \n",
    "5. If we increase both gamma and C, we can't be certain if the model becomes more complex or less complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TRUE responses are: 1, 2, 3, 4\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HParam Search over multiple hyperparameters\n",
    "What to do when you have 2+ HParam to decide?\n",
    "\n",
    "- Best validation error requires a hyperparameter search to **balance the fundamental (bias/variance) tradeoff**.\n",
    "- How do you check all values of all Hyper-Parameters to decide the **best set of HParams?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More on this next week. But if you cannot wait till then, you may look up the following:\n",
    "    - [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "    - [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Randomized hyperparameter search\n",
    "\n",
    "- Randomized hyperparameter optimization: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "- Samples configurations at random until certain budget (e.g., time) is exhausted.\n",
    "- Advantage: you can choose how many runs you'll do.\n",
    "- Advantage: you can restrict yourself less on what values you might try.\n",
    "- Advantage: Adding parameters that do not influence the performance does not affect efficiency.\n",
    "- Advantage: research shows this is generally a better idea than grid search, see image for intuition:\n",
    "\n",
    "![](../img/randomsearch_bergstra.png)\n",
    "\n",
    "Source: [Bergstra and Bengio, Random Search for Hyper-Parameter Optimization, JMLR 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n",
    "\n",
    "- You don't know in advance which hyperparameters are important for your problem.\n",
    "- But some of them might be unimportant.\n",
    "- In the left figure, 6 of the 9 searches are useless because they are only varying the unimportant parameter.\n",
    "- In the right figure, all 9 searches are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering [[video](https://youtu.be/xx9HlmzORRk)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**[Watch after the class if you haven't already]**\n",
    "- So far we have seen\n",
    "    - Three ML models (decision trees, $k$-NNs, SVMs)\n",
    "    - ML fundamentals (train-validation-test split, cross-validation, the fundamental tradeoff, the golden rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling \n",
    "\n",
    "Reminder from last session: KNN models are very sensitive to **scale** of features\n",
    "\n",
    "### Example: $k$-nearest neighbours on the Spotify dataset\n",
    "\n",
    "- In lab1 you used `DecisionTreeClassifier` to predict whether the user would like a particular song or not. \n",
    "- Can we use $k$-NN classifier for this task? \n",
    "- Intuition: To predict whether the user likes a particular song or not (query point) \n",
    "   - find the songs that are closest to the query point\n",
    "   - let them vote on the target\n",
    "   - take the majority vote as the target for the query point   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the code below, you need to download the dataset from [Kaggle](https://www.kaggle.com/geomack/spotifyclassification/download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"../data/spotify.csv\", index_col=0)\n",
    "train_df, test_df = train_test_split(spotify_df, test_size=0.20, random_state=123)\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    train_df[\"target\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    test_df[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "two_songs = X_train.sample(2, random_state=42)\n",
    "two_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "**[To study at home - The impact of feature scale on Euclidean distances]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "print('euclidean_distance of two songs: ', euclidean_distances(two_songs)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider only **one feature** of these two songs: `duration_ms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_songs_duration = two_songs[[\"duration_ms\"]]\n",
    "two_songs_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('euclidean_distance of two songs: ', euclidean_distances(two_songs)[0,1])\n",
    "print('euclidean_distance of two songs only duration: ', euclidean_distances(two_songs_duration)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The distance is completely dominated by the the features with larger values\n",
    "- The features with smaller values are being ignored. \n",
    "- Does it matter? \n",
    "    - Yes! Scale is based on how data was collected. \n",
    "    - Features on a smaller scale can be highly informative and there is no good reason to ignore them.\n",
    "    - We **sometimes** want our model to be **invariant to the scale.** (search online and learn about the term \"**invariance**\")\n",
    "- **Do decision trees suffer from the same problem?**\n",
    "  - No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling using `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- We'll use `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which is a `transformer`.   \n",
    "- Only focus on the syntax for now. We'll talk about scaling in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head() # original X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---- New class and new functions -----\n",
    "scaler = StandardScaler()  # create feature trasformer object\n",
    "scaler.fit(X_train)  # fitting the transformer on the train split\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)  # transforming the train split\n",
    "X_test_scaled = scaler.transform(X_test)  # transforming the test split\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, let's compare `X_train` with `X_train_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_train_scaled_df.head().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[X_train]\\t\\tmean(duration_ms)=', X_train.duration_ms.mean())\n",
    "print('[X_train_scaled]\\tmean(duration_ms)=', X_train_scaled_df.duration_ms.mean(), '\\n')\n",
    "\n",
    "print('[X_train]\\t\\tstd(duration_ms)=', X_train.duration_ms.std())\n",
    "print('[X_train_scaled]\\tstd(duration_ms)=', X_train_scaled_df.duration_ms.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "**StandardScaler**: Standardize features by removing the mean (`mean(feature) = 0`) and scaling to unit variance (`std(feature) = 1`).\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "**[For students to explore at home]**\n",
    "\n",
    "You can hand-calcualte the StandardScaler feature normalization with one line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's normalize value of the `energy` feature for the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['energy'].iloc[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train['energy'].iloc[0] - np.mean(X_train['energy']))/ X_train['energy'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index).head().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------------\n",
    "**[Recap/Summary to study on your own at home]**\n",
    "\n",
    "### `fit` and `transform` paradigm for \"Feature Transformers\"\n",
    "- `sklearn` uses `fit` and `transform` paradigms for feature transformations. \n",
    "- We `fit` the transformer on the train split and then transform the train split as well as the test split. \n",
    "- **We apply the same transformations on the test split.**\n",
    "\n",
    "(footnote: don't confuse these feature transformers with the ones that are popular these days in deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: estimators\n",
    "\n",
    "Suppose `model` is a classification or regression model. \n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train)\n",
    "X_train_predictions = model.predict(X_train)\n",
    "X_test_predictions = model.predict(X_test)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: feature transformers\n",
    "\n",
    "Suppose `transformer` is a transformer used to change the input representation, for example, to tackle missing values or to scales numeric features.\n",
    "\n",
    "```\n",
    "transformer.fit(X_train, [y_train])\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can pass `y_train` in `fit` but it's usually ignored. It allows you to pass it just to be consistent with usual usage of `sklearn`'s `fit` method.   \n",
    "- You can also carry out fitting and transforming in one call using `fit_transform`. But be mindful to **use it only on the train split** and **NOT on the test split**. \n",
    "\n",
    "------------------\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's check whether scaling makes any difference for $k$-NNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn_unscaled = KNeighborsClassifier()\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_unscaled.score(X_train, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_unscaled.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scaled = KNeighborsClassifier()\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_scaled.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Conclusion: **Results of KNN improved with feature normalization**.\n",
    "- **Note/Reminder (Golden Rule of ML)**: that I am a bit sloppy here and using the test set several times for teaching purposes. \n",
    "  - But when you build an ML pipeline, please do assessment on the test set only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common preprocessing techniques\n",
    "\n",
    "Some commonly performed feature transformation include:  \n",
    "- **Scaling**: Scaling of numeric features\n",
    "- **Imputation**: Tackling missing values\n",
    "- **One-hot encoding**: Tackling categorical variables      \n",
    "    \n",
    "\n",
    "We can have one lecture on each of them! In this lesson our goal is to getting familiar with them so that we can use them to build ML pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next part of this lecture, we'll build an ML pipeline using [California housing prices regression dataset](https://www.kaggle.com/harrywang/housing). In the process, we will talk about different feature transformations and how can we apply them so that we do not violate the golden rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset, splitting, and baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll be working on [California housing prices regression dataset](https://www.kaggle.com/harrywang/housing) to demonstrate these feature transformation techniques. The task is to predict median house values in Californian districts, given a number of features from these districts. If you are running the notebook on your own, you'll have to download the data and put it in the data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"../data/housing.csv\")\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's add some new features to the dataset which could help predicting the target: `median_house_value`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# rooms_per_household\n",
    "train_df = train_df.assign(\n",
    "    rooms_per_household=train_df[\"total_rooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    rooms_per_household=test_df[\"total_rooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "# bedrooms_per_household\n",
    "train_df = train_df.assign(\n",
    "    bedrooms_per_household=train_df[\"total_bedrooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    bedrooms_per_household=test_df[\"total_bedrooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "# population_per_household\n",
    "train_df = train_df.assign(\n",
    "    population_per_household=train_df[\"population\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    population_per_household=test_df[\"population\"] / test_df[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When is it OK to do things before splitting? \n",
    "\n",
    "- Here it would have been OK to add new features before splitting because we are not using any global information in the data but only looking at one row at a time. \n",
    "- **But just to be safe and to avoid accidentally breaking the golden rule, it's better to do it after splitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question**: Should we remove `total_rooms`, `total_bedrooms`, and `population` columns? \n",
    "    - **Answer:** Probably (maybe yes, maybe no). We should run an **empirical study** to answer this question:\n",
    "      - Train two models, \n",
    "        - one with these features\n",
    "        - one without these features\n",
    "      - Evaluate their performance\n",
    "      - Compare the results\n",
    "      - Decide whether to keep or remove the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.drop(columns = ['population', 'total_rooms', 'total_bedrooms'])\n",
    "# test_df =  test_df.drop(columns = ['population', 'total_rooms', 'total_bedrooms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature scales are quite different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have **one categorical feature** and all other features are **numeric features**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Seems like total_bedrooms column has some missing values. \n",
    "- This must have affected our new feature `bedrooms_per_household` as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## (optional)\n",
    "train_df.hist(bins=50, figsize=(20, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "------------\n",
    "**[To study at home]**\n",
    "Let's plot a few features into the same diagram:\n",
    "- longitude\n",
    "- latitude\n",
    "- population_per_household\n",
    "- median_house_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## (optional)\n",
    "train_df.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    alpha=0.4,\n",
    "    s=train_df[\"population_per_household\"],\n",
    "    figsize=(10, 7),\n",
    "    c=\"median_house_value\",\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True,\n",
    "    sharex=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we learned from the **EDA (Exploratory Data Analysis)**. \n",
    "\n",
    "- Some missing values in `total_bedrooms` column\n",
    "- Scales are quite different across columns. \n",
    "- Categorical variable `ocean_proximity`\n",
    "\n",
    "<br><br><br><br><br>\n",
    "**Conclusion:**\n",
    "Before starting the \"modeling (finding the right model to solve the problem)\", learn about the data via **EDA**\n",
    "<br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What all transformations we need to apply on the dataset? \n",
    "\n",
    "\n",
    "Read about [preprocessing techniques implemented in `scikit-learn`](https://scikit-learn.org/stable/modules/preprocessing.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are droping the categorical variable ocean_proximity for now. We'll come back to it in a bit.\n",
    "X_train = train_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's first run our baseline model `DummyRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store our results for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy=\"median\")\n",
    "results_dict[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "**Reminder to watch:** Imputation and scaling [[video](https://youtu.be/G2IXbVzKlt8)]\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "**Imputation** is the process of replacing **missing values** with substituted values.\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "# knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the problem? \n",
    "\n",
    "```\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "```\n",
    "\n",
    "- The classifier is not able to deal with missing values (NaNs).\n",
    "- What are possible ways to deal with the problem? \n",
    "    - Delete the rows? \n",
    "    - Replace them with some reasonable values?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `SimpleImputer` is a transformer in `sklearn` to deal with this problem. For example, \n",
    "    - You can impute missing values in categorical columns with the most frequent value.\n",
    "    - You can impute the missing values in numeric columns with the mean or median of the column.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train.sort_values(\"bedrooms_per_household\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "# ------------\n",
    "\n",
    "X_train_imp = imputer.transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's check whether the NaN values have been replaced or not\n",
    "- Note that `imputer.transform` returns an `numpy` array and not a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- This problem affects a large number of ML methods.\n",
    "- A number of approaches to this problem. We are going to look into two most popular ones.  \n",
    "\n",
    "| Approach | What it does | How to update $X$ (but see below!) | sklearn implementation | \n",
    "|---------|------------|-----------------------|----------------|\n",
    "| standardization | sets sample mean to $0$, s.d. to $1$   | `X -= np.mean(X,axis=0)`<br>`X /=  np.std(X,axis=0)` | [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------\n",
    "**Reference to Learn More:**\n",
    "- There are all sorts of articles on this; see, e.g. [here](http://www.dataminingblog.com/standardization-vs-normalization/) and [here](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc).\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference to Learn More:**\n",
    "- https://amueller.github.io/COMS4995-s19/slides/aml-05-preprocessing/#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # or try `StandardScaler()` at home\n",
    "X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Big difference in the KNN training performance after scaling the data. \n",
    "- But we saw last week that training score doesn't tell us much. We should look at the cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### (iClicker) Exercise 5.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/EMMJ**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "1. `StandardScaler` ensures a fixed range (i.e., minimum and maximum values) for the features. \n",
    "2. `StandardScaler` calculates mean and standard deviation for each feature separately. \n",
    "3. In general, it's a good idea to apply scaling on numeric features before training $k$-NN or SVM RBF models. \n",
    "4. The transformed feature values might be hard to interpret for humans.\n",
    "5. After applying `SimpleImputer`, the transformed data has a **different shape** than the original data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct TRUE answers are: 2, 3, 4\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Questions for class discussion \n",
    "\n",
    "Let's create some synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_classification\n",
    "\n",
    "# make synthetic data\n",
    "X, y = make_blobs(n_samples=100, centers=3, random_state=12, cluster_std=5)\n",
    "# split it into training and test sets\n",
    "X_train_toy, X_test_toy, y_train_toy, y_test_toy = train_test_split(\n",
    "    X, y, random_state=5, test_size=0.4)\n",
    "plt.scatter(X_train_toy[:, 0], X_train_toy[:, 1], label=\"Training set\", s=60)\n",
    "plt.scatter(\n",
    "    X_test_toy[:, 0], X_test_toy[:, 1], color=mglearn.cm2(1), label=\"Test set\", s=60\n",
    ")\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's transform the data using `StandardScaler` and examine how the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_transformed = scaler.fit_transform(X_train_toy)\n",
    "test_transformed = scaler.transform(X_test_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 0].mean(), X_train_toy[:, 0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 1].mean(), X_train_toy[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_functions import plot_original_scaled\n",
    "plot_original_scaled(X_train_toy, X_test_toy, train_transformed, test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Bad methodology 1: Scaling the data separately (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_toy)\n",
    "train_scaled_bad_demo = scaler.transform(X_train_toy)\n",
    "\n",
    "# ---------- INCORRECT CODE ------------------\n",
    "scaler_test = StandardScaler()  # Creating a separate object for scaling test data\n",
    "scaler_test.fit(X_test_toy)  # Calling fit on the test data\n",
    "test_scaled_bad_demo = scaler_test.transform(\n",
    "    X_test_toy\n",
    ")  # Transforming the test data using the scaler fit on test data\n",
    "# ---------------------------------------------\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_scaled_bad_demo, y_train_toy)\n",
    "print(f\"Training score: {knn.score(train_scaled_bad_demo, y_train_toy):.2f}\")\n",
    "print(f\"Test score: {knn.score(test_scaled_bad_demo, y_test_toy):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_original_scaled(\n",
    "    train_transformed,\n",
    "    test_transformed,\n",
    "    train_scaled_bad_demo,\n",
    "    test_scaled_bad_demo,\n",
    "    title_transformed=\"Improperly transformed\",\n",
    "    title_original=\"correctly transformed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is anything wrong in methodology 1? If yes, what is it?  \n",
    "- What are the mean and standard deviation of columns in `X_train_toy` and `X_test_toy`?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "-----------\n",
    "**[Follow these cells at home to understand why fitting scaler on TEST data is WRONG]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 0].mean(), X_train_toy[:, 0].std() # mean and std of column 1 in X_train_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 1].mean(), X_train_toy[:, 1].std() # mean and std of column 2 in X_train_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the mean and standard deviation of columns in `X_test_toy`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_toy[:, 0].mean(), X_test_toy[:, 0].std() # mean and std of column 1 in X_test_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_toy[:, 1].mean(), X_test_toy[:, 1].std() # mean and std of column 2 in X_train_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bad methodology 2: Scaling the data together (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_toy.shape, X_test_toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# join the train and test sets back together\n",
    "XX = np.vstack((X_train_toy, X_test_toy))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(XX)\n",
    "XX_scaled = scaler.transform(XX)\n",
    "XX_scaled_train = XX_scaled[:X_train_toy.shape[0]]\n",
    "XX_scaled_test = XX_scaled[X_train_toy.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(XX_scaled_train, y_train_toy)\n",
    "print(f\"Training score: {knn.score(XX_scaled_train, y_train_toy):.2f}\")  # Misleading score\n",
    "print(f\"Test score: {knn.score(XX_scaled_test, y_test_toy):.2f}\")  # Misleading score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Is anything wrong in methodology 2? If yes, what is it? \n",
    "- What's are the mean and std of `X_train_toy` vs. `XX`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_original_scaled(\n",
    "    train_transformed,\n",
    "    test_transformed,\n",
    "    XX_scaled_train,\n",
    "    XX_scaled_test,\n",
    "    title_transformed=\"Improperly transformed\",\n",
    "    title_original=\"correctly transformed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no big difference but they are not the same:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(train_transformed - XX_scaled_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You violated the Golde Rule of ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "-----------\n",
    "**[Follow these cells at home to understand why fitting scaler on TEST data is WRONG]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 0].mean(), X_train_toy[:, 0].std() # mean and std of column 1 in X_train_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toy[:, 1].mean(), X_train_toy[:, 1].std() # mean and std of column 2 in X_train_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the mean and standard deviation of columns in `XX`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX[:, 0].mean(), XX[:, 0].std() # mean and std of column 1 in XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX[:, 1].mean(), XX[:, 1].std() # mean and std of column 2 in XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a noticeable difference in the transformed data but if the test set is large it might influence the mean and standard deviation significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Methodology 3: Cross validation with already preprocessed data (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_toy)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_toy)\n",
    "X_test_scaled = scaler.transform(X_test_toy)\n",
    "\n",
    "# Is this OK? (focus on `X_train_scaled`)\n",
    "scores = cross_validate(knn, X_train_scaled, y_train_toy, return_train_score=True)\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is there anything wrong in methodology 3? Are we breaking the **golden rule** here?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_improper_processing\n",
    "plot_improper_processing(\"kNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from plotting_functions import plot_proper_processing\n",
    "plot_proper_processing(\"kNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How to carry out cross-validation + feature transformation? \n",
    "\n",
    "- Last week we saw that cross validation is a better way to get a realistic assessment of the model. \n",
    "- Let's try cross-validation with transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imp)\n",
    "X_train_scaled = scaler.transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "scores = cross_validate(knn, X_train_scaled, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do you see any problem here? \n",
    "- Are we applying `fit_transform` on train portion and `transform` on validation portion in each fold?  \n",
    "    - Here you might be allowing information from the validation set to **leak** into the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You need to apply the **SAME** preprocessing steps to train/validation.\n",
    "- With many different transformations and cross validation the code gets unwieldy very quickly. \n",
    "- Likely to make mistakes and \"leak\" information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- In these examples our test accuracies look fine, but our methodology is flawed.\n",
    "- Implications can be significant in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pipelines\n",
    "\n",
    "Can we do this in a more elegant and organized way?\n",
    "\n",
    "- YES!! Using [`scikit-learn Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "- [`scikit-learn Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) allows you to define a \"pipeline\" of transformers with a final estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's combine the preprocessing and model with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Simple example of a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", KNeighborsRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Syntax: pass in a list of steps.\n",
    "- The last step should be a **model/classifier/regressor**.\n",
    "- All the earlier steps should be **transformers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative and more compact syntax: `make_pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shorthand for `Pipeline` constructor\n",
    "- Does not permit naming steps\n",
    "- Instead the names of steps are set to lowercase of their types automatically; `StandardScaler()` would be named as `standardscaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), \n",
    "    StandardScaler(), \n",
    "    KNeighborsRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are passing `X_train` and **not** the imputed or scaled data here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you call `fit` on the pipeline, it carries out the following steps:\n",
    "\n",
    "- Fit `SimpleImputer` on `X_train`\n",
    "- Transform `X_train` using the fit `SimpleImputer` to create `X_train_imp`\n",
    "- Fit `StandardScaler` on `X_train_imp`\n",
    "- Transform `X_train_imp` using the fit `StandardScaler` to create `X_train_imp_scaled`\n",
    "- Fit the model (`KNeighborsRegressor` in our case) on `X_train_imp_scaled`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are passing original data to `predict` as well. This time the pipeline is carrying out following steps:\n",
    "- Transform `X_train` using the fit `SimpleImputer` to create `X_train_imp`\n",
    "- Transform `X_train_imp` using the fit `StandardScaler` to create `X_train_imp_scaled`\n",
    "- Predict using the fit model (`KNeighborsRegressor` in our case) on `X_train_imp_scaled`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../img/pipeline.png' width=\"800\">\n",
    "    \n",
    "[Source](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try cross-validation with our pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that `pipe` is passed instead of the model\n",
    "results_dict[\"imp + scaling + knn\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using a `Pipeline` takes care of applying the `fit_transform` on the train portion and only `transform` on the validation portion in each fold.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Categorical features [[video](https://youtu.be/2mJ9rAhMMl0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that we had dropped the categorical feature `ocean_proximity` feature from the dataframe. But it could potentially be a useful feature in this task. \n",
    "\n",
    "- Let's create our `X_train` and and `X_test` again by keeping the feature in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"median_house_value\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's try to build a `KNeighborRegressor` on this data using our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This **failed** because we have non-numeric data. \n",
    "- Imagine how $k$-NN would calculate distances when you have non-numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we use this feature in the model? \n",
    "- In `scikit-learn`, most algorithms require numeric inputs.\n",
    "- Decision trees could theoretically work with categorical features.  \n",
    "    - However, the sklearn implementation does not support this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are the options? \n",
    "\n",
    "- Drop the column (not recommended)\n",
    "    - If you know that the column is not relevant to the target in any way you may drop it. \n",
    "- We can transform categorical features to numeric ones so that we can use them in the model.     \n",
    "    - [Ordinal encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) (occasionally recommended)\n",
    "    - One-hot encoding (recommended in most cases) (this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"language\": [\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"Mandarin\",\n",
    "            \"French\",\n",
    "            \"Spanish\",\n",
    "            \"Mandarin\",\n",
    "            \"Hindi\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "X_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordinal encoding (occasionally recommended)\n",
    "\n",
    "- Here we simply assign an integer to each of our unique categorical labels. \n",
    "- We can use sklearn's [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# ---------- New Class ---------------\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(X_toy)\n",
    "X_toy_ord = enc.transform(X_toy)\n",
    "# -----------------------------------\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=X_toy_ord,\n",
    "    columns=[\"language_enc\"],\n",
    "    index=X_toy.index,\n",
    ")\n",
    "pd.concat([X_toy, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the problem with this approach? \n",
    "- We have imposed ordinality on the categorical data.\n",
    "- For example, imagine when you are calculating distances. Is it fair to say that French and Hindi are closer than French and Spanish? \n",
    "- In general, label encoding is useful if there is ordinality in your data and capturing it is important for your problem, e.g., `[cold, warm, hot]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-hot encoding (OHE)\n",
    "- Create new binary columns to represent our categories.\n",
    "- If we have $c$ categories in our column.\n",
    "    - We create $c$ new binary columns to represent those categories.\n",
    "- Example: Imagine a language column which has the information on whether you \n",
    "\n",
    "- We can use sklearn's [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "**One-hot encoding is called one-hot because only one of the newly created features is 1 for each data point.**\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "enc.fit(X_toy)\n",
    "X_toy_ohe = enc.transform(X_toy)\n",
    "df_onehot = pd.DataFrame(\n",
    "    data=X_toy_ohe,\n",
    "    columns=enc.get_feature_names_out([\"language\"]),\n",
    "    index=X_toy.index,\n",
    ")\n",
    "pd.concat([X_toy, df_onehot], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "-----------\n",
    "**[Try the following cells at home]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Let's do it on our housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype=\"int\")\n",
    "ohe.fit(X_train[[\"ocean_proximity\"]])\n",
    "X_imp_ohe_train = ohe.transform(X_train[[\"ocean_proximity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look at the new features created using `categories_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "transformed_ohe = pd.DataFrame(\n",
    "    data=X_imp_ohe_train,\n",
    "    columns=ohe.get_feature_names_out([\"ocean_proximity\"]),\n",
    "    index=X_train.index,\n",
    ")\n",
    "transformed_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "One-hot encoded variables are also referred to as **dummy variables**.   \n",
    "You will often see people using [`get_dummies` method of pandas](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to convert categorical variables into dummy variables.  \n",
    "That said, using `sklearn`'s `OneHotEncoder` has the advantage of making it easy to treat training and test set in a consistent way.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓❓ Question for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 5.2\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "Should we be careful of the **order** we put each transformation and model in a pipeline?  \n",
    "Does the **order** matter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct answer: YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- Motivation for preprocessing\n",
    "- Common preprocessing steps\n",
    "    - Imputation \n",
    "    - Scaling\n",
    "    - One-hot encoding\n",
    "- Golden rule in the context of preprocessing\n",
    "- Building simple supervised machine learning pipelines using `sklearn.pipeline.make_pipeline`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Different transformations on different columns\n",
    "- How do we put this together with other columns in the data before fitting the regressor? \n",
    "- Before we fit our regressor, we want to apply different transformations on different columns \n",
    "    - Numeric columns\n",
    "        - imputation \n",
    "        - scaling         \n",
    "    - Categorical columns \n",
    "        - imputation \n",
    "        - one-hot encoding        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Coming up: sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)!!** "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
